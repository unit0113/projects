from collections import deque
from random import shuffle
import torch
import numpy as np
from matplotlib import pyplot as plt


def init_grid(size=(10,)):
    grid = torch.randn(*size)
    grid[grid > 0] = 1
    grid[grid <= 0] = 0
    grid = grid.byte() # Converts floats to binary
    return grid


def gen_params(N, size):
    # Generates list of paramater vectors for NN
    ret = []
    for _ in range(N):
        vec = torch.randn(size) / 10.
        vec.requires_grad = True
        ret.append(vec)
    return ret


def qfunc(s,theta,layers=[(4,20),(20,2)], afn=torch.tanh):
    l1n = layers[0] 
    l1s = np.prod(l1n) # Takes first tuple in layers and multiplies those numbers to get the subset of the theta vector to use as the first layer
    theta_1 = theta[0:l1s].reshape(l1n) # Reshapes theta vector subset into a matrix for use as first layer in NN
    l2n = layers[1]
    l2s = np.prod(l2n)
    theta_2 = theta[l1s:l2s+l1s].reshape(l2n)
    bias = torch.ones((1,theta_1.shape[1]))
    l1 = s @ theta_1 + bias # First layer computation
    l1 = torch.nn.functional.elu(l1)
    l2 = afn(l1 @ theta_2) # Default tanh activation fxn
    return l2.flatten()


def softmax_policy(qvals,temp=0.9): # Takes in Q value vector and outputs an action, 0=up, 1=down
    soft = torch.exp(qvals/temp) / torch.sum(torch.exp(qvals/temp)) # Softmax fxn
    action = torch.multinomial(soft,1) # Sample action from softmax probabilities
    return action


def get_coords(grid,j): # Converts flattened index to x, y coords
    x = int(np.floor(j / grid.shape[0]))
    y = int(j - x * grid.shape[0])
    return x, y


def get_reward_2d(action,action_mean):
    r = (action*(action_mean-action/2)).sum()/action.sum() # Based on how different the action is from the mean field action
    return torch.tanh(5 * r) # Scale to [-1, 1]


def get_substate(b): # converts binary number to one-hot encoded vector
    s = torch.zeros(2) 
    if b > 0: # if up, else down
        s[1] = 1
    else:
        s[0] = 1
    return s


def mean_action(grid,j):
    x, y = get_coords(grid,j) # Converts 1D index to coords
    action_mean = torch.zeros(2) # Initialize action mean vector
    for i in [-1, 0, 1]: # Find neighbors
        for k in [-1, 0, 1]:
            if i == k == 0:
                continue
            x_, y_ = x + i, y + k
            x_ = x_ if x_ >= 0 else grid.shape[0] - 1
            y_ = y_ if y_ >= 0 else grid.shape[1] - 1
            x_ = x_ if x_ <  grid.shape[0] else 0
            y_ = y_ if y_ < grid.shape[1] else 0
            cur_n = grid[x_,y_]
            s = get_substate(cur_n) #D
            action_mean += s
    action_mean /= action_mean.sum() #E
    return action_mean





def main():
    size = (10,10)
    J = np.prod(size) 
    hid_layer = 10
    layers = [(2,hid_layer),(hid_layer,2)]
    params = gen_params(1,2*hid_layer+hid_layer*2)
    grid = init_grid(size=size)
    grid_ = grid.clone()
    grid__ = grid.clone()

    epochs = 75
    lr = 0.0001
    num_iter = 3 # Number of iters to eliminate initial randomness
    losses = [ [] for i in range(size[0])] # Losses for each agent
    replay_size = 50 # Total stored experiences
    replay = deque(maxlen=replay_size) # Experience replay buffer
    batch_size = 10 # Number of replays sampled
    gamma = 0.9 # Discount factor
    losses = [[] for i in range(J)]

    for _ in range(epochs): 
        act_means = torch.zeros((J,2)) # Stores mean field for all agents
        q_next = torch.zeros(J) # Stores Q value for next state after taking an action
        for _ in range(num_iter): # Dilute initial randomness
            for j in range(J): # Iterate through all agents in grid
                action_mean = mean_action(grid_,j).detach()
                act_means[j] = action_mean.clone()
                qvals = qfunc(action_mean.detach(),params[0], layers=layers)
                action = softmax_policy(qvals.detach(), temp=0.5)
                grid__[get_coords(grid_,j)] = action
                q_next[j] = torch.max(qvals).detach()
            grid_.data = grid__.data
        grid.data = grid_.data
        actions = torch.stack([get_substate(a.item()) for a in grid.flatten()])
        rewards = torch.stack([get_reward_2d(actions[j],act_means[j]) for j in range(J)])
        exp = (actions,rewards,act_means,q_next) # Adds experience to replay buffer
        replay.append(exp)
        shuffle(replay)
        if len(replay) > batch_size: # Start training
            ids = np.random.randint(low=0, high=len(replay), size=batch_size) # Sample replay buffer
            exps = [replay[idx] for idx in ids]
            for j in range(J):
                jacts = torch.stack([ex[0][j] for ex in exps]).detach()
                jrewards = torch.stack([ex[1][j] for ex in exps]).detach()
                jmeans = torch.stack([ex[2][j] for ex in exps]).detach()
                vs = torch.stack([ex[3][j] for ex in exps]).detach()
                qvals = torch.stack([ qfunc(jmeans[h].detach(),params[0],layers=layers) \
                                    for h in range(batch_size)])
                target = qvals.clone().detach()
                target[:,torch.argmax(jacts,dim=1)] = jrewards + gamma * vs
                loss = torch.sum(torch.pow(qvals - target.detach(),2))
                losses[j].append(loss.item())
                loss.backward()
                with torch.no_grad():
                    params[0] = params[0] - lr * params[0].grad
                params[0].requires_grad = True

    fig,ax = plt.subplots(2,1)
    fig.set_size_inches(10,10)
    ax[0].plot(np.array(losses).mean(axis=0))
    ax[1].imshow(grid)
    plt.show()


if __name__ == '__main__':
    main()