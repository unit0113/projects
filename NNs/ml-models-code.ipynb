{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Normal Equation\n",
    "\n",
    "theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "\n",
    "# Linear Regression with MSE\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1/n_samples) * 2 * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * 2 * np.sum(y_pred - y)\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_approx = np.dot(X, self.weights) + self.bias\n",
    "        return y_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with Log Loss\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear_model)\n",
    "            dw = (1/n_samples) * 2 * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * 2 * np.sum(y_pred - y)\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.bias * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear_model)\n",
    "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "        return y_pred_cls\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=201):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None; self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_ = np.where(y <= 0, -1, 1)  # y is -1 or 1\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        for i in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = (y_[idx] * (np.dot(x_i, self.w) + self.b) >= 1)\n",
    "                if condition:\n",
    "                    dw = 2 * self.lambda_param * self.w \n",
    "                    db = 0\n",
    "                else:\n",
    "                    dw = 2 * self.lambda_param * self.w - y_[idx] * x_i\n",
    "                    db = y_[idx]\n",
    "                self.w -= self.lr * dw\n",
    "                self.b -= self.lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier \n",
    "# Implemented with information gain + early stopping + min_samples_split + max_depth + max_features\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Take in y-labels for all samples associated with a node\"\"\"\n",
    "    hist = np.bincount(y)                                                              # count num of occurrences of each non-neg int value in array\n",
    "    p = hist / len(y)\n",
    "    return -np.sum([pi*np.log2(pi) for pi in p if pi > 0])                             # sum only real values greater than 0\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Each node stores 2 split parameters: the feature to split on and the threshold to split by\n",
    "    If the node is a parent, also point to a left and right node\n",
    "    If the node is a leaf, store the most common class label among the training samples as value\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self) -> bool:\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, max_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Grow the tree\"\"\"\n",
    "        self.max_features = X.shape[1] if not self.max_features else min(X.shape[1], self.max_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively grow the tree\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria, if stopped create and return a leaf node\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Else, continue building out the tree using a greedy search\n",
    "        feature_idxs = np.random.choice(n_features, self.max_features, replace=False)  # randomly choose feature subset\n",
    "        best_feature, best_threshold = self._best_criteria(X, y, feature_idxs)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]                                  # extract the most common label\n",
    "        return most_common\n",
    "\n",
    "    def _best_criteria(self, X, y, feature_idxs):\n",
    "        \"\"\"For each feature selected by feature_idxs, loop through all available labels, save the pair (feature, label)\n",
    "        with the highest information gain as our split criteria\"\"\"\n",
    "        split_idx, split_threshold, best_inf_gain = None, None, -1\n",
    "        for feat_idx in feature_idxs:\n",
    "            X_col = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_col)\n",
    "            for thresh in thresholds:\n",
    "                inf_gain = self._information_gain(y, X_col, thresh)                 # technically split on info gain not entropy, but same result\n",
    "                if inf_gain > best_inf_gain:\n",
    "                    best_inf_gain = inf_gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thresh\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_threshold):\n",
    "        # Calculate parent node entropy\n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # Generate node split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_threshold)\n",
    "\n",
    "        # Calculate child node entropies\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        n, n_left, n_right = len(y), len(left_idxs), len(right_idxs)\n",
    "        entropy_left, entropy_right = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_left/n) * entropy_left + (n_right/n) * entropy_right      # CART cost func using entropy\n",
    "\n",
    "        # Return information gain\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def _split(self, X_column, split_threshold):\n",
    "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Traverse the tree\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Use DecisionTree instances as weak learners\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "def most_common_label(y):\n",
    "    \"\"\"Plurality voting scheme\"\"\"\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]  # most common label\n",
    "    return most_common\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"Takes as input num of trees to ensemble, plus all DecisionTree hyperparameters\"\"\"\n",
    "    def __init__(self, n_trees=100, min_samples_split=2, max_depth=100, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(self.min_samples_split, self.max_depth, self.max_features)\n",
    "            X_sample, y_sample = bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])  # each row is a tree's predictions of X\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "        y_pred = np.array([most_common_label(tree_pred) for tree_pred in tree_preds])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted_labels = [self._predict(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_idx = np.argsort(distances)[:self.k]  # indices of k nearest neigbors\n",
    "        k_neighbor_labels = [self.y_train[idx] for idx in k_idx]\n",
    "        most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# Implemented with log-trick and Gaussian PDF from scratch\n",
    "\n",
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        \n",
    "        # For each feature/class combo, calculate mean and var\n",
    "        self._means = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._vars = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        \n",
    "        # For each class, calculate prior\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]                                # extract X samples that are class `c`\n",
    "            self._means[idx, :] = np.mean(X_c, axis=0)     # find X_c feature means\n",
    "            self._vars[idx, :] = np.var(X_c, axis=0)       # find X_c feature variances\n",
    "            self._priors[idx] = X_c.shape[0] / n_samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        # Calculate posterior probability for each class\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            class_cond = np.sum(np.log(self._gaussian_pdf(idx, x)))\n",
    "            posterior = class_cond + prior\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # Return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _gaussian_pdf(self, class_idx, x):\n",
    "        means = self._means[class_idx, :]\n",
    "        vars = self._vars[class_idx, :]\n",
    "\n",
    "        # Gaussian PDF formula\n",
    "        numerator = np.exp(-((x - means) ** 2) / (2 * vars))\n",
    "        denominator = np.sqrt(2 * np.pi * vars)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "# Doesn't implement transform method to predict on test data, instead retrain on data each time predict called\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters\n",
    "        self.plot_steps = plot_steps\n",
    "\n",
    "        # List of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "\n",
    "        # The centers (mean feature vector) for each cluster\n",
    "        self.centroids = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # Initialize centroids\n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "\n",
    "        # Optimize clusters\n",
    "        for _ in range(self.max_iters):\n",
    "            # Assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "            # Calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "\n",
    "            # Check if clusters have changed\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "\n",
    "            # Plot each iteration of KMeans\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "        # Classify samples as the index of their clusters\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "\n",
    "    def _create_clusters(self, centroids):\n",
    "        # Assign the samples to the closest centroids to create clusters\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        # Distance of the current sample to each centroid\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "\n",
    "    def _get_centroids(self, clusters):\n",
    "        # Assign mean value of clusters to centroids\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster], axis=0)  # remember each cluster is just a list of sample idxes that belong to that cluster\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "\n",
    "    def _is_converged(self, centroids_old, centroids_new):\n",
    "        # Distances between each old and new centroids, for all centroids\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids_new[i]) for i in range(self.K)]\n",
    "        return sum(distances) == 0\n",
    "\n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        # Each sample will get the label of the cluster it was assigned to\n",
    "        labels = np.empty(self.n_samples)\n",
    "\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_idx\n",
    "        return labels\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
