{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pierian-Data-Logo.PNG\">\n",
    "<br>\n",
    "<strong><center>Copyright 2019. Created by Jose Marcial Portilla.</center></strong>\n",
    "\n",
    "# Generating Text (encoded variables)\n",
    "\n",
    "We saw how to generate continuous values, now let's see how to generalize this to generate categorical sequences (such as words or letters).\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open shakespeare text file and read in data as `text`\n",
    "with open('../Data/shakespeare.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = tuple(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('k',\n",
       " 'F',\n",
       " 'c',\n",
       " 'f',\n",
       " 'r',\n",
       " '[',\n",
       " 'p',\n",
       " 'J',\n",
       " '>',\n",
       " 'j',\n",
       " 'z',\n",
       " '_',\n",
       " 'i',\n",
       " 'I',\n",
       " '}',\n",
       " 'h',\n",
       " '-',\n",
       " 'y',\n",
       " 'Z',\n",
       " '<',\n",
       " 's',\n",
       " 'A',\n",
       " 'B',\n",
       " 'E',\n",
       " ')',\n",
       " '1',\n",
       " 'W',\n",
       " '|',\n",
       " '\"',\n",
       " 'a',\n",
       " 'P',\n",
       " '4',\n",
       " 'U',\n",
       " '6',\n",
       " 'Q',\n",
       " 'o',\n",
       " '(',\n",
       " '`',\n",
       " '.',\n",
       " 'S',\n",
       " ';',\n",
       " '?',\n",
       " 'V',\n",
       " '2',\n",
       " '&',\n",
       " 'X',\n",
       " 'L',\n",
       " 'n',\n",
       " 'b',\n",
       " 'l',\n",
       " 'O',\n",
       " '9',\n",
       " '!',\n",
       " '\\n',\n",
       " '8',\n",
       " 'D',\n",
       " 'H',\n",
       " ',',\n",
       " 'T',\n",
       " 'C',\n",
       " 'x',\n",
       " ':',\n",
       " \"'\",\n",
       " 'M',\n",
       " 'q',\n",
       " 'e',\n",
       " 'w',\n",
       " 'R',\n",
       " 't',\n",
       " '0',\n",
       " '7',\n",
       " 'd',\n",
       " 'N',\n",
       " '5',\n",
       " 'K',\n",
       " 'v',\n",
       " 'g',\n",
       " 'Y',\n",
       " ']',\n",
       " 'u',\n",
       " 'G',\n",
       " 'm',\n",
       " ' ',\n",
       " '3')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'k',\n",
       " 1: 'F',\n",
       " 2: 'c',\n",
       " 3: 'f',\n",
       " 4: 'r',\n",
       " 5: '[',\n",
       " 6: 'p',\n",
       " 7: 'J',\n",
       " 8: '>',\n",
       " 9: 'j',\n",
       " 10: 'z',\n",
       " 11: '_',\n",
       " 12: 'i',\n",
       " 13: 'I',\n",
       " 14: '}',\n",
       " 15: 'h',\n",
       " 16: '-',\n",
       " 17: 'y',\n",
       " 18: 'Z',\n",
       " 19: '<',\n",
       " 20: 's',\n",
       " 21: 'A',\n",
       " 22: 'B',\n",
       " 23: 'E',\n",
       " 24: ')',\n",
       " 25: '1',\n",
       " 26: 'W',\n",
       " 27: '|',\n",
       " 28: '\"',\n",
       " 29: 'a',\n",
       " 30: 'P',\n",
       " 31: '4',\n",
       " 32: 'U',\n",
       " 33: '6',\n",
       " 34: 'Q',\n",
       " 35: 'o',\n",
       " 36: '(',\n",
       " 37: '`',\n",
       " 38: '.',\n",
       " 39: 'S',\n",
       " 40: ';',\n",
       " 41: '?',\n",
       " 42: 'V',\n",
       " 43: '2',\n",
       " 44: '&',\n",
       " 45: 'X',\n",
       " 46: 'L',\n",
       " 47: 'n',\n",
       " 48: 'b',\n",
       " 49: 'l',\n",
       " 50: 'O',\n",
       " 51: '9',\n",
       " 52: '!',\n",
       " 53: '\\n',\n",
       " 54: '8',\n",
       " 55: 'D',\n",
       " 56: 'H',\n",
       " 57: ',',\n",
       " 58: 'T',\n",
       " 59: 'C',\n",
       " 60: 'x',\n",
       " 61: ':',\n",
       " 62: \"'\",\n",
       " 63: 'M',\n",
       " 64: 'q',\n",
       " 65: 'e',\n",
       " 66: 'w',\n",
       " 67: 'R',\n",
       " 68: 't',\n",
       " 69: '0',\n",
       " 70: '7',\n",
       " 71: 'd',\n",
       " 72: 'N',\n",
       " 73: '5',\n",
       " 74: 'K',\n",
       " 75: 'v',\n",
       " 76: 'g',\n",
       " 77: 'Y',\n",
       " 78: ']',\n",
       " 79: 'u',\n",
       " 80: 'G',\n",
       " 81: 'm',\n",
       " 82: ' ',\n",
       " 83: '3'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: i for i, char in encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 0,\n",
       " 'F': 1,\n",
       " 'c': 2,\n",
       " 'f': 3,\n",
       " 'r': 4,\n",
       " '[': 5,\n",
       " 'p': 6,\n",
       " 'J': 7,\n",
       " '>': 8,\n",
       " 'j': 9,\n",
       " 'z': 10,\n",
       " '_': 11,\n",
       " 'i': 12,\n",
       " 'I': 13,\n",
       " '}': 14,\n",
       " 'h': 15,\n",
       " '-': 16,\n",
       " 'y': 17,\n",
       " 'Z': 18,\n",
       " '<': 19,\n",
       " 's': 20,\n",
       " 'A': 21,\n",
       " 'B': 22,\n",
       " 'E': 23,\n",
       " ')': 24,\n",
       " '1': 25,\n",
       " 'W': 26,\n",
       " '|': 27,\n",
       " '\"': 28,\n",
       " 'a': 29,\n",
       " 'P': 30,\n",
       " '4': 31,\n",
       " 'U': 32,\n",
       " '6': 33,\n",
       " 'Q': 34,\n",
       " 'o': 35,\n",
       " '(': 36,\n",
       " '`': 37,\n",
       " '.': 38,\n",
       " 'S': 39,\n",
       " ';': 40,\n",
       " '?': 41,\n",
       " 'V': 42,\n",
       " '2': 43,\n",
       " '&': 44,\n",
       " 'X': 45,\n",
       " 'L': 46,\n",
       " 'n': 47,\n",
       " 'b': 48,\n",
       " 'l': 49,\n",
       " 'O': 50,\n",
       " '9': 51,\n",
       " '!': 52,\n",
       " '\\n': 53,\n",
       " '8': 54,\n",
       " 'D': 55,\n",
       " 'H': 56,\n",
       " ',': 57,\n",
       " 'T': 58,\n",
       " 'C': 59,\n",
       " 'x': 60,\n",
       " ':': 61,\n",
       " \"'\": 62,\n",
       " 'M': 63,\n",
       " 'q': 64,\n",
       " 'e': 65,\n",
       " 'w': 66,\n",
       " 'R': 67,\n",
       " 't': 68,\n",
       " '0': 69,\n",
       " '7': 70,\n",
       " 'd': 71,\n",
       " 'N': 72,\n",
       " '5': 73,\n",
       " 'K': 74,\n",
       " 'v': 75,\n",
       " 'g': 76,\n",
       " 'Y': 77,\n",
       " ']': 78,\n",
       " 'u': 79,\n",
       " 'G': 80,\n",
       " 'm': 81,\n",
       " ' ': 82,\n",
       " '3': 83}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = [encoder[char] for char in text]\n",
    "encoded_text = np.array(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82,\n",
       "       82, 82, 82, 82, 82, 25, 53, 82, 82,  1,  4, 35, 81, 82,  3, 29, 12,\n",
       "        4, 65, 20, 68, 82,  2,  4, 65, 29, 68, 79,  4, 65, 20, 82, 66])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[82]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches(char_arr, batch_size, seq_len):\n",
    "    \n",
    "    '''\n",
    "    char_arr : array of characters\n",
    "    batch_size : size of batch\n",
    "    seq_len : length of training sequence\n",
    "    '''\n",
    "    \n",
    "    total_batch_size = batch_size * seq_len\n",
    "  \n",
    "    num_batches = len(char_arr) / total_batch_size\n",
    "    \n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    \n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot_arr = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot_arr[np.arange(one_hot_arr.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, n_hidden=256, n_layers=2,drop_prob=0.5, lr=0.001):\n",
    "        '''\n",
    "        LSTM --> DropOut ---> FC\n",
    "        '''\n",
    "        \n",
    "        # For init_hidden method\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, n_hidden, n_layers, dropout=drop_prob, batch_first=True)   \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, input_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "\n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Pass LSTM_Output through Dropout\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "\n",
    "        # Reshape output using view for FC\n",
    "        # https://stackoverflow.com/questions/48915810/pytorch-contiguous\n",
    "        drop_output = drop_output.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        # Pass through FC\n",
    "        fc_final_out = self.fc(drop_output)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return fc_final_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
      ")\n",
      "Epoch: 1/100... Step: 50... Loss: 3.1955... Val Loss: 3.2117\n",
      "Epoch: 1/100... Step: 100... Loss: 3.1437... Val Loss: 3.1279\n",
      "Epoch: 1/100... Step: 150... Loss: 2.8496... Val Loss: 2.8282\n",
      "Epoch: 1/100... Step: 200... Loss: 2.5257... Val Loss: 2.4636\n",
      "Epoch: 1/100... Step: 250... Loss: 2.3174... Val Loss: 2.2507\n",
      "Epoch: 1/100... Step: 300... Loss: 2.1953... Val Loss: 2.1250\n",
      "Epoch: 1/100... Step: 350... Loss: 2.0662... Val Loss: 2.0319\n",
      "Epoch: 2/100... Step: 400... Loss: 2.0426... Val Loss: 1.9600\n",
      "Epoch: 2/100... Step: 450... Loss: 1.9731... Val Loss: 1.8998\n",
      "Epoch: 2/100... Step: 500... Loss: 1.8989... Val Loss: 1.8398\n",
      "Epoch: 2/100... Step: 550... Loss: 1.8564... Val Loss: 1.7976\n",
      "Epoch: 2/100... Step: 600... Loss: 1.8277... Val Loss: 1.7582\n",
      "Epoch: 2/100... Step: 650... Loss: 1.7905... Val Loss: 1.7206\n",
      "Epoch: 2/100... Step: 700... Loss: 1.7769... Val Loss: 1.7128\n",
      "Epoch: 2/100... Step: 750... Loss: 1.6947... Val Loss: 1.6653\n",
      "Epoch: 3/100... Step: 800... Loss: 1.6832... Val Loss: 1.6445\n",
      "Epoch: 3/100... Step: 850... Loss: 1.6524... Val Loss: 1.6163\n",
      "Epoch: 3/100... Step: 900... Loss: 1.6624... Val Loss: 1.5985\n",
      "Epoch: 3/100... Step: 950... Loss: 1.6077... Val Loss: 1.5839\n",
      "Epoch: 3/100... Step: 1000... Loss: 1.5910... Val Loss: 1.5627\n",
      "Epoch: 3/100... Step: 1050... Loss: 1.6453... Val Loss: 1.5501\n",
      "Epoch: 3/100... Step: 1100... Loss: 1.5816... Val Loss: 1.5351\n",
      "Epoch: 4/100... Step: 1150... Loss: 1.5464... Val Loss: 1.5329\n",
      "Epoch: 4/100... Step: 1200... Loss: 1.5494... Val Loss: 1.5170\n",
      "Epoch: 4/100... Step: 1250... Loss: 1.5125... Val Loss: 1.5015\n",
      "Epoch: 4/100... Step: 1300... Loss: 1.5201... Val Loss: 1.4957\n",
      "Epoch: 4/100... Step: 1350... Loss: 1.5181... Val Loss: 1.4866\n",
      "Epoch: 4/100... Step: 1400... Loss: 1.5316... Val Loss: 1.4762\n",
      "Epoch: 4/100... Step: 1450... Loss: 1.5281... Val Loss: 1.4682\n",
      "Epoch: 4/100... Step: 1500... Loss: 1.5037... Val Loss: 1.4628\n",
      "Epoch: 5/100... Step: 1550... Loss: 1.4894... Val Loss: 1.4532\n",
      "Epoch: 5/100... Step: 1600... Loss: 1.4650... Val Loss: 1.4458\n",
      "Epoch: 5/100... Step: 1650... Loss: 1.4345... Val Loss: 1.4402\n",
      "Epoch: 5/100... Step: 1700... Loss: 1.4463... Val Loss: 1.4382\n",
      "Epoch: 5/100... Step: 1750... Loss: 1.4466... Val Loss: 1.4252\n",
      "Epoch: 5/100... Step: 1800... Loss: 1.3836... Val Loss: 1.4215\n",
      "Epoch: 5/100... Step: 1850... Loss: 1.4322... Val Loss: 1.4174\n",
      "Epoch: 5/100... Step: 1900... Loss: 1.4484... Val Loss: 1.4148\n",
      "Epoch: 6/100... Step: 1950... Loss: 1.4100... Val Loss: 1.4097\n",
      "Epoch: 6/100... Step: 2000... Loss: 1.3991... Val Loss: 1.4020\n",
      "Epoch: 6/100... Step: 2050... Loss: 1.3846... Val Loss: 1.3963\n",
      "Epoch: 6/100... Step: 2100... Loss: 1.3909... Val Loss: 1.3978\n",
      "Epoch: 6/100... Step: 2150... Loss: 1.4219... Val Loss: 1.3891\n",
      "Epoch: 6/100... Step: 2200... Loss: 1.4078... Val Loss: 1.3881\n",
      "Epoch: 6/100... Step: 2250... Loss: 1.3924... Val Loss: 1.3824\n",
      "Epoch: 7/100... Step: 2300... Loss: 1.3549... Val Loss: 1.3830\n",
      "Epoch: 7/100... Step: 2350... Loss: 1.3635... Val Loss: 1.3810\n",
      "Epoch: 7/100... Step: 2400... Loss: 1.3492... Val Loss: 1.3703\n",
      "Epoch: 7/100... Step: 2450... Loss: 1.3498... Val Loss: 1.3699\n",
      "Epoch: 7/100... Step: 2500... Loss: 1.3220... Val Loss: 1.3722\n",
      "Epoch: 7/100... Step: 2550... Loss: 1.4017... Val Loss: 1.3666\n",
      "Epoch: 7/100... Step: 2600... Loss: 1.3519... Val Loss: 1.3662\n",
      "Epoch: 7/100... Step: 2650... Loss: 1.3494... Val Loss: 1.3631\n",
      "Epoch: 8/100... Step: 2700... Loss: 1.3313... Val Loss: 1.3596\n",
      "Epoch: 8/100... Step: 2750... Loss: 1.3473... Val Loss: 1.3548\n",
      "Epoch: 8/100... Step: 2800... Loss: 1.3190... Val Loss: 1.3501\n",
      "Epoch: 8/100... Step: 2850... Loss: 1.2625... Val Loss: 1.3469\n",
      "Epoch: 8/100... Step: 2900... Loss: 1.2863... Val Loss: 1.3482\n",
      "Epoch: 8/100... Step: 2950... Loss: 1.3422... Val Loss: 1.3467\n",
      "Epoch: 8/100... Step: 3000... Loss: 1.3066... Val Loss: 1.3454\n",
      "Epoch: 8/100... Step: 3050... Loss: 1.3250... Val Loss: 1.3403\n",
      "Epoch: 9/100... Step: 3100... Loss: 1.2781... Val Loss: 1.3408\n",
      "Epoch: 9/100... Step: 3150... Loss: 1.3426... Val Loss: 1.3313\n",
      "Epoch: 9/100... Step: 3200... Loss: 1.2523... Val Loss: 1.3284\n",
      "Epoch: 9/100... Step: 3250... Loss: 1.2829... Val Loss: 1.3350\n",
      "Epoch: 9/100... Step: 3300... Loss: 1.2941... Val Loss: 1.3309\n",
      "Epoch: 9/100... Step: 3350... Loss: 1.3022... Val Loss: 1.3322\n",
      "Epoch: 9/100... Step: 3400... Loss: 1.2550... Val Loss: 1.3312\n",
      "Epoch: 10/100... Step: 3450... Loss: 1.3092... Val Loss: 1.3254\n",
      "Epoch: 10/100... Step: 3500... Loss: 1.2826... Val Loss: 1.3218\n",
      "Epoch: 10/100... Step: 3550... Loss: 1.2984... Val Loss: 1.3181\n",
      "Epoch: 10/100... Step: 3600... Loss: 1.2431... Val Loss: 1.3167\n",
      "Epoch: 10/100... Step: 3650... Loss: 1.2385... Val Loss: 1.3185\n",
      "Epoch: 10/100... Step: 3700... Loss: 1.2723... Val Loss: 1.3173\n",
      "Epoch: 10/100... Step: 3750... Loss: 1.2505... Val Loss: 1.3143\n",
      "Epoch: 10/100... Step: 3800... Loss: 1.2559... Val Loss: 1.3147\n",
      "Epoch: 11/100... Step: 3850... Loss: 1.2689... Val Loss: 1.3161\n",
      "Epoch: 11/100... Step: 3900... Loss: 1.2440... Val Loss: 1.3133\n",
      "Epoch: 11/100... Step: 3950... Loss: 1.2711... Val Loss: 1.3105\n",
      "Epoch: 11/100... Step: 4000... Loss: 1.2588... Val Loss: 1.3104\n",
      "Epoch: 11/100... Step: 4050... Loss: 1.2648... Val Loss: 1.3059\n",
      "Epoch: 11/100... Step: 4100... Loss: 1.2721... Val Loss: 1.3025\n",
      "Epoch: 11/100... Step: 4150... Loss: 1.2420... Val Loss: 1.3077\n",
      "Epoch: 11/100... Step: 4200... Loss: 1.2849... Val Loss: 1.3036\n",
      "Epoch: 12/100... Step: 4250... Loss: 1.2248... Val Loss: 1.3037\n",
      "Epoch: 12/100... Step: 4300... Loss: 1.2284... Val Loss: 1.2984\n",
      "Epoch: 12/100... Step: 4350... Loss: 1.1781... Val Loss: 1.2979\n",
      "Epoch: 12/100... Step: 4400... Loss: 1.2532... Val Loss: 1.2980\n",
      "Epoch: 12/100... Step: 4450... Loss: 1.2259... Val Loss: 1.2962\n",
      "Epoch: 12/100... Step: 4500... Loss: 1.2200... Val Loss: 1.2976\n",
      "Epoch: 12/100... Step: 4550... Loss: 1.2066... Val Loss: 1.2951\n",
      "Epoch: 13/100... Step: 4600... Loss: 1.2667... Val Loss: 1.2992\n",
      "Epoch: 13/100... Step: 4650... Loss: 1.2611... Val Loss: 1.2954\n",
      "Epoch: 13/100... Step: 4700... Loss: 1.2087... Val Loss: 1.2881\n",
      "Epoch: 13/100... Step: 4750... Loss: 1.2287... Val Loss: 1.2901\n",
      "Epoch: 13/100... Step: 4800... Loss: 1.2300... Val Loss: 1.2912\n",
      "Epoch: 13/100... Step: 4850... Loss: 1.2262... Val Loss: 1.2901\n",
      "Epoch: 13/100... Step: 4900... Loss: 1.2464... Val Loss: 1.2898\n",
      "Epoch: 13/100... Step: 4950... Loss: 1.2201... Val Loss: 1.2875\n",
      "Epoch: 14/100... Step: 5000... Loss: 1.2059... Val Loss: 1.2868\n",
      "Epoch: 14/100... Step: 5050... Loss: 1.2444... Val Loss: 1.2843\n",
      "Epoch: 14/100... Step: 5100... Loss: 1.1987... Val Loss: 1.2826\n",
      "Epoch: 14/100... Step: 5150... Loss: 1.2145... Val Loss: 1.2843\n",
      "Epoch: 14/100... Step: 5200... Loss: 1.2004... Val Loss: 1.2847\n",
      "Epoch: 14/100... Step: 5250... Loss: 1.2170... Val Loss: 1.2832\n",
      "Epoch: 14/100... Step: 5300... Loss: 1.1864... Val Loss: 1.2844\n",
      "Epoch: 15/100... Step: 5350... Loss: 1.1780... Val Loss: 1.2844\n",
      "Epoch: 15/100... Step: 5400... Loss: 1.2365... Val Loss: 1.2812\n",
      "Epoch: 15/100... Step: 5450... Loss: 1.2032... Val Loss: 1.2795\n",
      "Epoch: 15/100... Step: 5500... Loss: 1.2050... Val Loss: 1.2718\n",
      "Epoch: 15/100... Step: 5550... Loss: 1.2134... Val Loss: 1.2795\n",
      "Epoch: 15/100... Step: 5600... Loss: 1.2231... Val Loss: 1.2807\n",
      "Epoch: 15/100... Step: 5650... Loss: 1.2055... Val Loss: 1.2811\n",
      "Epoch: 15/100... Step: 5700... Loss: 1.2590... Val Loss: 1.2797\n",
      "Epoch: 16/100... Step: 5750... Loss: 1.2137... Val Loss: 1.2771\n",
      "Epoch: 16/100... Step: 5800... Loss: 1.2441... Val Loss: 1.2790\n",
      "Epoch: 16/100... Step: 5850... Loss: 1.2081... Val Loss: 1.2728\n",
      "Epoch: 16/100... Step: 5900... Loss: 1.2062... Val Loss: 1.2711\n",
      "Epoch: 16/100... Step: 5950... Loss: 1.2403... Val Loss: 1.2730\n",
      "Epoch: 16/100... Step: 6000... Loss: 1.1744... Val Loss: 1.2735\n",
      "Epoch: 16/100... Step: 6050... Loss: 1.2004... Val Loss: 1.2726\n",
      "Epoch: 16/100... Step: 6100... Loss: 1.1695... Val Loss: 1.2719\n",
      "Epoch: 17/100... Step: 6150... Loss: 1.1825... Val Loss: 1.2688\n",
      "Epoch: 17/100... Step: 6200... Loss: 1.1829... Val Loss: 1.2663\n",
      "Epoch: 17/100... Step: 6250... Loss: 1.1637... Val Loss: 1.2741\n",
      "Epoch: 17/100... Step: 6300... Loss: 1.1904... Val Loss: 1.2715\n",
      "Epoch: 17/100... Step: 6350... Loss: 1.1759... Val Loss: 1.2718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100... Step: 6400... Loss: 1.2139... Val Loss: 1.2689\n",
      "Epoch: 17/100... Step: 6450... Loss: 1.1988... Val Loss: 1.2696\n",
      "Epoch: 18/100... Step: 6500... Loss: 1.1614... Val Loss: 1.2704\n",
      "Epoch: 18/100... Step: 6550... Loss: 1.1721... Val Loss: 1.2690\n",
      "Epoch: 18/100... Step: 6600... Loss: 1.2283... Val Loss: 1.2668\n",
      "Epoch: 18/100... Step: 6650... Loss: 1.1910... Val Loss: 1.2662\n",
      "Epoch: 18/100... Step: 6700... Loss: 1.1584... Val Loss: 1.2705\n",
      "Epoch: 18/100... Step: 6750... Loss: 1.1958... Val Loss: 1.2720\n",
      "Epoch: 18/100... Step: 6800... Loss: 1.2159... Val Loss: 1.2693\n",
      "Epoch: 18/100... Step: 6850... Loss: 1.1737... Val Loss: 1.2675\n",
      "Epoch: 19/100... Step: 6900... Loss: 1.1971... Val Loss: 1.2642\n",
      "Epoch: 19/100... Step: 6950... Loss: 1.1968... Val Loss: 1.2671\n",
      "Epoch: 19/100... Step: 7000... Loss: 1.1919... Val Loss: 1.2652\n",
      "Epoch: 19/100... Step: 7050... Loss: 1.1311... Val Loss: 1.2657\n",
      "Epoch: 19/100... Step: 7100... Loss: 1.1908... Val Loss: 1.2669\n",
      "Epoch: 19/100... Step: 7150... Loss: 1.1867... Val Loss: 1.2646\n",
      "Epoch: 19/100... Step: 7200... Loss: 1.2043... Val Loss: 1.2668\n",
      "Epoch: 19/100... Step: 7250... Loss: 1.1802... Val Loss: 1.2656\n",
      "Epoch: 20/100... Step: 7300... Loss: 1.1504... Val Loss: 1.2661\n",
      "Epoch: 20/100... Step: 7350... Loss: 1.1995... Val Loss: 1.2641\n",
      "Epoch: 20/100... Step: 7400... Loss: 1.1131... Val Loss: 1.2631\n",
      "Epoch: 20/100... Step: 7450... Loss: 1.1690... Val Loss: 1.2637\n",
      "Epoch: 20/100... Step: 7500... Loss: 1.1973... Val Loss: 1.2635\n",
      "Epoch: 20/100... Step: 7550... Loss: 1.1634... Val Loss: 1.2561\n",
      "Epoch: 20/100... Step: 7600... Loss: 1.2050... Val Loss: 1.2622\n",
      "Epoch: 21/100... Step: 7650... Loss: 1.1620... Val Loss: 1.2600\n",
      "Epoch: 21/100... Step: 7700... Loss: 1.1831... Val Loss: 1.2620\n",
      "Epoch: 21/100... Step: 7750... Loss: 1.1490... Val Loss: 1.2578\n",
      "Epoch: 21/100... Step: 7800... Loss: 1.1625... Val Loss: 1.2590\n",
      "Epoch: 21/100... Step: 7850... Loss: 1.1348... Val Loss: 1.2606\n",
      "Epoch: 21/100... Step: 7900... Loss: 1.1943... Val Loss: 1.2671\n",
      "Epoch: 21/100... Step: 7950... Loss: 1.1775... Val Loss: 1.2631\n",
      "Epoch: 21/100... Step: 8000... Loss: 1.1668... Val Loss: 1.2679\n",
      "Epoch: 22/100... Step: 8050... Loss: 1.1807... Val Loss: 1.2595\n",
      "Epoch: 22/100... Step: 8100... Loss: 1.1395... Val Loss: 1.2599\n",
      "Epoch: 22/100... Step: 8150... Loss: 1.1885... Val Loss: 1.2603\n",
      "Epoch: 22/100... Step: 8200... Loss: 1.1146... Val Loss: 1.2596\n",
      "Epoch: 22/100... Step: 8250... Loss: 1.1644... Val Loss: 1.2655\n",
      "Epoch: 22/100... Step: 8300... Loss: 1.1643... Val Loss: 1.2576\n",
      "Epoch: 22/100... Step: 8350... Loss: 1.1563... Val Loss: 1.2586\n",
      "Epoch: 22/100... Step: 8400... Loss: 1.1545... Val Loss: 1.2611\n",
      "Epoch: 23/100... Step: 8450... Loss: 1.1603... Val Loss: 1.2613\n",
      "Epoch: 23/100... Step: 8500... Loss: 1.1296... Val Loss: 1.2579\n",
      "Epoch: 23/100... Step: 8550... Loss: 1.1140... Val Loss: 1.2587\n",
      "Epoch: 23/100... Step: 8600... Loss: 1.1530... Val Loss: 1.2627\n",
      "Epoch: 23/100... Step: 8650... Loss: 1.1549... Val Loss: 1.2593\n",
      "Epoch: 23/100... Step: 8700... Loss: 1.1499... Val Loss: 1.2623\n",
      "Epoch: 23/100... Step: 8750... Loss: 1.1303... Val Loss: 1.2610\n",
      "Epoch: 24/100... Step: 8800... Loss: 1.1599... Val Loss: 1.2605\n",
      "Epoch: 24/100... Step: 8850... Loss: 1.1639... Val Loss: 1.2604\n",
      "Epoch: 24/100... Step: 8900... Loss: 1.1495... Val Loss: 1.2560\n",
      "Epoch: 24/100... Step: 8950... Loss: 1.1376... Val Loss: 1.2572\n",
      "Epoch: 24/100... Step: 9000... Loss: 1.0964... Val Loss: 1.2640\n",
      "Epoch: 24/100... Step: 9050... Loss: 1.1457... Val Loss: 1.2563\n",
      "Epoch: 24/100... Step: 9100... Loss: 1.1271... Val Loss: 1.2597\n",
      "Epoch: 24/100... Step: 9150... Loss: 1.1353... Val Loss: 1.2605\n",
      "Epoch: 25/100... Step: 9200... Loss: 1.1359... Val Loss: 1.2548\n",
      "Epoch: 25/100... Step: 9250... Loss: 1.1671... Val Loss: 1.2591\n",
      "Epoch: 25/100... Step: 9300... Loss: 1.1630... Val Loss: 1.2552\n",
      "Epoch: 25/100... Step: 9350... Loss: 1.1451... Val Loss: 1.2580\n",
      "Epoch: 25/100... Step: 9400... Loss: 1.1501... Val Loss: 1.2571\n",
      "Epoch: 25/100... Step: 9450... Loss: 1.1377... Val Loss: 1.2563\n",
      "Epoch: 25/100... Step: 9500... Loss: 1.1370... Val Loss: 1.2586\n",
      "Epoch: 25/100... Step: 9550... Loss: 1.2119... Val Loss: 1.2565\n",
      "Epoch: 26/100... Step: 9600... Loss: 1.1499... Val Loss: 1.2577\n",
      "Epoch: 26/100... Step: 9650... Loss: 1.1093... Val Loss: 1.2553\n",
      "Epoch: 26/100... Step: 9700... Loss: 1.1169... Val Loss: 1.2541\n",
      "Epoch: 26/100... Step: 9750... Loss: 1.1415... Val Loss: 1.2578\n",
      "Epoch: 26/100... Step: 9800... Loss: 1.1795... Val Loss: 1.2533\n",
      "Epoch: 26/100... Step: 9850... Loss: 1.1516... Val Loss: 1.2532\n",
      "Epoch: 26/100... Step: 9900... Loss: 1.1263... Val Loss: 1.2553\n",
      "Epoch: 27/100... Step: 9950... Loss: 1.1278... Val Loss: 1.2556\n",
      "Epoch: 27/100... Step: 10000... Loss: 1.2000... Val Loss: 1.2546\n",
      "Epoch: 27/100... Step: 10050... Loss: 1.1541... Val Loss: 1.2475\n",
      "Epoch: 27/100... Step: 10100... Loss: 1.1562... Val Loss: 1.2497\n",
      "Epoch: 27/100... Step: 10150... Loss: 1.1352... Val Loss: 1.2542\n",
      "Epoch: 27/100... Step: 10200... Loss: 1.1587... Val Loss: 1.2471\n",
      "Epoch: 27/100... Step: 10250... Loss: 1.1758... Val Loss: 1.2586\n",
      "Epoch: 27/100... Step: 10300... Loss: 1.1020... Val Loss: 1.2542\n",
      "Epoch: 28/100... Step: 10350... Loss: 1.0886... Val Loss: 1.2566\n",
      "Epoch: 28/100... Step: 10400... Loss: 1.1260... Val Loss: 1.2508\n",
      "Epoch: 28/100... Step: 10450... Loss: 1.1463... Val Loss: 1.2533\n",
      "Epoch: 28/100... Step: 10500... Loss: 1.1394... Val Loss: 1.2536\n",
      "Epoch: 28/100... Step: 10550... Loss: 1.1183... Val Loss: 1.2511\n",
      "Epoch: 28/100... Step: 10600... Loss: 1.1988... Val Loss: 1.2514\n",
      "Epoch: 28/100... Step: 10650... Loss: 1.1460... Val Loss: 1.2566\n",
      "Epoch: 29/100... Step: 10700... Loss: 1.1030... Val Loss: 1.2551\n",
      "Epoch: 29/100... Step: 10750... Loss: 1.1438... Val Loss: 1.2543\n",
      "Epoch: 29/100... Step: 10800... Loss: 1.1211... Val Loss: 1.2528\n",
      "Epoch: 29/100... Step: 10850... Loss: 1.1168... Val Loss: 1.2466\n",
      "Epoch: 29/100... Step: 10900... Loss: 1.1266... Val Loss: 1.2508\n",
      "Epoch: 29/100... Step: 10950... Loss: 1.1626... Val Loss: 1.2532\n",
      "Epoch: 29/100... Step: 11000... Loss: 1.1470... Val Loss: 1.2488\n",
      "Epoch: 29/100... Step: 11050... Loss: 1.1468... Val Loss: 1.2555\n",
      "Epoch: 30/100... Step: 11100... Loss: 1.1500... Val Loss: 1.2475\n",
      "Epoch: 30/100... Step: 11150... Loss: 1.1334... Val Loss: 1.2519\n",
      "Epoch: 30/100... Step: 11200... Loss: 1.1123... Val Loss: 1.2469\n",
      "Epoch: 30/100... Step: 11250... Loss: 1.1318... Val Loss: 1.2500\n",
      "Epoch: 30/100... Step: 11300... Loss: 1.1330... Val Loss: 1.2543\n",
      "Epoch: 30/100... Step: 11350... Loss: 1.1031... Val Loss: 1.2515\n",
      "Epoch: 30/100... Step: 11400... Loss: 1.1389... Val Loss: 1.2564\n",
      "Epoch: 30/100... Step: 11450... Loss: 1.1533... Val Loss: 1.2522\n",
      "Epoch: 31/100... Step: 11500... Loss: 1.1195... Val Loss: 1.2551\n",
      "Epoch: 31/100... Step: 11550... Loss: 1.1208... Val Loss: 1.2469\n",
      "Epoch: 31/100... Step: 11600... Loss: 1.1058... Val Loss: 1.2549\n",
      "Epoch: 31/100... Step: 11650... Loss: 1.1337... Val Loss: 1.2482\n",
      "Epoch: 31/100... Step: 11700... Loss: 1.1554... Val Loss: 1.2529\n",
      "Epoch: 31/100... Step: 11750... Loss: 1.1626... Val Loss: 1.2454\n",
      "Epoch: 31/100... Step: 11800... Loss: 1.1382... Val Loss: 1.2497\n",
      "Epoch: 32/100... Step: 11850... Loss: 1.1070... Val Loss: 1.2543\n",
      "Epoch: 32/100... Step: 11900... Loss: 1.1381... Val Loss: 1.2562\n",
      "Epoch: 32/100... Step: 11950... Loss: 1.1041... Val Loss: 1.2501\n",
      "Epoch: 32/100... Step: 12000... Loss: 1.1192... Val Loss: 1.2503\n",
      "Epoch: 32/100... Step: 12050... Loss: 1.1099... Val Loss: 1.2540\n",
      "Epoch: 32/100... Step: 12100... Loss: 1.1673... Val Loss: 1.2563\n",
      "Epoch: 32/100... Step: 12150... Loss: 1.1302... Val Loss: 1.2493\n",
      "Epoch: 32/100... Step: 12200... Loss: 1.1279... Val Loss: 1.2535\n",
      "Epoch: 33/100... Step: 12250... Loss: 1.1224... Val Loss: 1.2525\n",
      "Epoch: 33/100... Step: 12300... Loss: 1.1255... Val Loss: 1.2492\n",
      "Epoch: 33/100... Step: 12350... Loss: 1.1236... Val Loss: 1.2487\n",
      "Epoch: 33/100... Step: 12400... Loss: 1.0677... Val Loss: 1.2499\n",
      "Epoch: 33/100... Step: 12450... Loss: 1.0874... Val Loss: 1.2500\n",
      "Epoch: 33/100... Step: 12500... Loss: 1.1504... Val Loss: 1.2504\n",
      "Epoch: 33/100... Step: 12550... Loss: 1.1206... Val Loss: 1.2512\n",
      "Epoch: 33/100... Step: 12600... Loss: 1.1276... Val Loss: 1.2499\n",
      "Epoch: 34/100... Step: 12650... Loss: 1.0881... Val Loss: 1.2504\n",
      "Epoch: 34/100... Step: 12700... Loss: 1.1518... Val Loss: 1.2459\n",
      "Epoch: 34/100... Step: 12750... Loss: 1.0712... Val Loss: 1.2495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100... Step: 12800... Loss: 1.1188... Val Loss: 1.2486\n",
      "Epoch: 34/100... Step: 12850... Loss: 1.1197... Val Loss: 1.2448\n",
      "Epoch: 34/100... Step: 12900... Loss: 1.1157... Val Loss: 1.2470\n",
      "Epoch: 34/100... Step: 12950... Loss: 1.0806... Val Loss: 1.2511\n",
      "Epoch: 35/100... Step: 13000... Loss: 1.1290... Val Loss: 1.2536\n",
      "Epoch: 35/100... Step: 13050... Loss: 1.1134... Val Loss: 1.2523\n",
      "Epoch: 35/100... Step: 13100... Loss: 1.1285... Val Loss: 1.2485\n",
      "Epoch: 35/100... Step: 13150... Loss: 1.0755... Val Loss: 1.2487\n",
      "Epoch: 35/100... Step: 13200... Loss: 1.0842... Val Loss: 1.2481\n",
      "Epoch: 35/100... Step: 13250... Loss: 1.1181... Val Loss: 1.2483\n",
      "Epoch: 35/100... Step: 13300... Loss: 1.0919... Val Loss: 1.2495\n",
      "Epoch: 35/100... Step: 13350... Loss: 1.1035... Val Loss: 1.2561\n",
      "Epoch: 36/100... Step: 13400... Loss: 1.1165... Val Loss: 1.2500\n",
      "Epoch: 36/100... Step: 13450... Loss: 1.0881... Val Loss: 1.2524\n",
      "Epoch: 36/100... Step: 13500... Loss: 1.1278... Val Loss: 1.2519\n",
      "Epoch: 36/100... Step: 13550... Loss: 1.1022... Val Loss: 1.2533\n",
      "Epoch: 36/100... Step: 13600... Loss: 1.1192... Val Loss: 1.2540\n",
      "Epoch: 36/100... Step: 13650... Loss: 1.1232... Val Loss: 1.2504\n",
      "Epoch: 36/100... Step: 13700... Loss: 1.1064... Val Loss: 1.2498\n",
      "Epoch: 36/100... Step: 13750... Loss: 1.1443... Val Loss: 1.2531\n",
      "Epoch: 37/100... Step: 13800... Loss: 1.0979... Val Loss: 1.2485\n",
      "Epoch: 37/100... Step: 13850... Loss: 1.0921... Val Loss: 1.2451\n",
      "Epoch: 37/100... Step: 13900... Loss: 1.0491... Val Loss: 1.2479\n",
      "Epoch: 37/100... Step: 13950... Loss: 1.1194... Val Loss: 1.2541\n",
      "Epoch: 37/100... Step: 14000... Loss: 1.0918... Val Loss: 1.2488\n",
      "Epoch: 37/100... Step: 14050... Loss: 1.1003... Val Loss: 1.2485\n",
      "Epoch: 37/100... Step: 14100... Loss: 1.0872... Val Loss: 1.2514\n",
      "Epoch: 38/100... Step: 14150... Loss: 1.1370... Val Loss: 1.2507\n",
      "Epoch: 38/100... Step: 14200... Loss: 1.1302... Val Loss: 1.2496\n",
      "Epoch: 38/100... Step: 14250... Loss: 1.0827... Val Loss: 1.2486\n",
      "Epoch: 38/100... Step: 14300... Loss: 1.1051... Val Loss: 1.2484\n",
      "Epoch: 38/100... Step: 14350... Loss: 1.1103... Val Loss: 1.2487\n",
      "Epoch: 38/100... Step: 14400... Loss: 1.1019... Val Loss: 1.2507\n",
      "Epoch: 38/100... Step: 14450... Loss: 1.1249... Val Loss: 1.2505\n",
      "Epoch: 38/100... Step: 14500... Loss: 1.0998... Val Loss: 1.2529\n",
      "Epoch: 39/100... Step: 14550... Loss: 1.0952... Val Loss: 1.2481\n",
      "Epoch: 39/100... Step: 14600... Loss: 1.1244... Val Loss: 1.2522\n",
      "Epoch: 39/100... Step: 14650... Loss: 1.0866... Val Loss: 1.2469\n",
      "Epoch: 39/100... Step: 14700... Loss: 1.0949... Val Loss: 1.2475\n",
      "Epoch: 39/100... Step: 14750... Loss: 1.0870... Val Loss: 1.2510\n",
      "Epoch: 39/100... Step: 14800... Loss: 1.1174... Val Loss: 1.2453\n",
      "Epoch: 39/100... Step: 14850... Loss: 1.0794... Val Loss: 1.2468\n",
      "Epoch: 40/100... Step: 14900... Loss: 1.0670... Val Loss: 1.2536\n",
      "Epoch: 40/100... Step: 14950... Loss: 1.1169... Val Loss: 1.2497\n",
      "Epoch: 40/100... Step: 15000... Loss: 1.1000... Val Loss: 1.2483\n",
      "Epoch: 40/100... Step: 15050... Loss: 1.0926... Val Loss: 1.2487\n",
      "Epoch: 40/100... Step: 15100... Loss: 1.0940... Val Loss: 1.2490\n",
      "Epoch: 40/100... Step: 15150... Loss: 1.1186... Val Loss: 1.2485\n",
      "Epoch: 40/100... Step: 15200... Loss: 1.0967... Val Loss: 1.2521\n",
      "Epoch: 40/100... Step: 15250... Loss: 1.1538... Val Loss: 1.2530\n",
      "Epoch: 41/100... Step: 15300... Loss: 1.0988... Val Loss: 1.2485\n",
      "Epoch: 41/100... Step: 15350... Loss: 1.1353... Val Loss: 1.2536\n",
      "Epoch: 41/100... Step: 15400... Loss: 1.1028... Val Loss: 1.2472\n",
      "Epoch: 41/100... Step: 15450... Loss: 1.1048... Val Loss: 1.2452\n",
      "Epoch: 41/100... Step: 15500... Loss: 1.1292... Val Loss: 1.2452\n",
      "Epoch: 41/100... Step: 15550... Loss: 1.0763... Val Loss: 1.2501\n",
      "Epoch: 41/100... Step: 15600... Loss: 1.1034... Val Loss: 1.2533\n",
      "Epoch: 41/100... Step: 15650... Loss: 1.0821... Val Loss: 1.2544\n",
      "Epoch: 42/100... Step: 15700... Loss: 1.0862... Val Loss: 1.2533\n",
      "Epoch: 42/100... Step: 15750... Loss: 1.0966... Val Loss: 1.2521\n",
      "Epoch: 42/100... Step: 15800... Loss: 1.0720... Val Loss: 1.2475\n",
      "Epoch: 42/100... Step: 15850... Loss: 1.0939... Val Loss: 1.2472\n",
      "Epoch: 42/100... Step: 15900... Loss: 1.0769... Val Loss: 1.2473\n",
      "Epoch: 42/100... Step: 15950... Loss: 1.1255... Val Loss: 1.2510\n",
      "Epoch: 42/100... Step: 16000... Loss: 1.1011... Val Loss: 1.2528\n",
      "Epoch: 43/100... Step: 16050... Loss: 1.0725... Val Loss: 1.2557\n",
      "Epoch: 43/100... Step: 16100... Loss: 1.0860... Val Loss: 1.2515\n",
      "Epoch: 43/100... Step: 16150... Loss: 1.1417... Val Loss: 1.2499\n",
      "Epoch: 43/100... Step: 16200... Loss: 1.0972... Val Loss: 1.2488\n",
      "Epoch: 43/100... Step: 16250... Loss: 1.0673... Val Loss: 1.2541\n",
      "Epoch: 43/100... Step: 16300... Loss: 1.1049... Val Loss: 1.2505\n",
      "Epoch: 43/100... Step: 16350... Loss: 1.1142... Val Loss: 1.2490\n",
      "Epoch: 43/100... Step: 16400... Loss: 1.0890... Val Loss: 1.2550\n",
      "Epoch: 44/100... Step: 16450... Loss: 1.1048... Val Loss: 1.2529\n",
      "Epoch: 44/100... Step: 16500... Loss: 1.1028... Val Loss: 1.2515\n",
      "Epoch: 44/100... Step: 16550... Loss: 1.1007... Val Loss: 1.2490\n",
      "Epoch: 44/100... Step: 16600... Loss: 1.0594... Val Loss: 1.2503\n",
      "Epoch: 44/100... Step: 16650... Loss: 1.1009... Val Loss: 1.2525\n",
      "Epoch: 44/100... Step: 16700... Loss: 1.0975... Val Loss: 1.2497\n",
      "Epoch: 44/100... Step: 16750... Loss: 1.1070... Val Loss: 1.2519\n",
      "Epoch: 44/100... Step: 16800... Loss: 1.1048... Val Loss: 1.2523\n",
      "Epoch: 45/100... Step: 16850... Loss: 1.0754... Val Loss: 1.2520\n",
      "Epoch: 45/100... Step: 16900... Loss: 1.1259... Val Loss: 1.2504\n",
      "Epoch: 45/100... Step: 16950... Loss: 1.0384... Val Loss: 1.2523\n",
      "Epoch: 45/100... Step: 17000... Loss: 1.0864... Val Loss: 1.2502\n",
      "Epoch: 45/100... Step: 17050... Loss: 1.1115... Val Loss: 1.2520\n",
      "Epoch: 45/100... Step: 17100... Loss: 1.0847... Val Loss: 1.2498\n",
      "Epoch: 45/100... Step: 17150... Loss: 1.1157... Val Loss: 1.2515\n",
      "Epoch: 46/100... Step: 17200... Loss: 1.0818... Val Loss: 1.2529\n",
      "Epoch: 46/100... Step: 17250... Loss: 1.0986... Val Loss: 1.2570\n",
      "Epoch: 46/100... Step: 17300... Loss: 1.0735... Val Loss: 1.2546\n",
      "Epoch: 46/100... Step: 17350... Loss: 1.0980... Val Loss: 1.2477\n",
      "Epoch: 46/100... Step: 17400... Loss: 1.0608... Val Loss: 1.2514\n",
      "Epoch: 46/100... Step: 17450... Loss: 1.1113... Val Loss: 1.2533\n",
      "Epoch: 46/100... Step: 17500... Loss: 1.0963... Val Loss: 1.2514\n",
      "Epoch: 46/100... Step: 17550... Loss: 1.0893... Val Loss: 1.2528\n",
      "Epoch: 47/100... Step: 17600... Loss: 1.1010... Val Loss: 1.2512\n",
      "Epoch: 47/100... Step: 17650... Loss: 1.0620... Val Loss: 1.2515\n",
      "Epoch: 47/100... Step: 17700... Loss: 1.1100... Val Loss: 1.2530\n",
      "Epoch: 47/100... Step: 17750... Loss: 1.0432... Val Loss: 1.2544\n",
      "Epoch: 47/100... Step: 17800... Loss: 1.0911... Val Loss: 1.2531\n",
      "Epoch: 47/100... Step: 17850... Loss: 1.0954... Val Loss: 1.2514\n",
      "Epoch: 47/100... Step: 17900... Loss: 1.0801... Val Loss: 1.2527\n",
      "Epoch: 47/100... Step: 17950... Loss: 1.0843... Val Loss: 1.2545\n",
      "Epoch: 48/100... Step: 18000... Loss: 1.0824... Val Loss: 1.2525\n",
      "Epoch: 48/100... Step: 18050... Loss: 1.0585... Val Loss: 1.2463\n",
      "Epoch: 48/100... Step: 18100... Loss: 1.0505... Val Loss: 1.2522\n",
      "Epoch: 48/100... Step: 18150... Loss: 1.0804... Val Loss: 1.2545\n",
      "Epoch: 48/100... Step: 18200... Loss: 1.0821... Val Loss: 1.2476\n",
      "Epoch: 48/100... Step: 18250... Loss: 1.0778... Val Loss: 1.2511\n",
      "Epoch: 48/100... Step: 18300... Loss: 1.0445... Val Loss: 1.2477\n",
      "Epoch: 49/100... Step: 18350... Loss: 1.0997... Val Loss: 1.2484\n",
      "Epoch: 49/100... Step: 18400... Loss: 1.0843... Val Loss: 1.2532\n",
      "Epoch: 49/100... Step: 18450... Loss: 1.0886... Val Loss: 1.2514\n",
      "Epoch: 49/100... Step: 18500... Loss: 1.0616... Val Loss: 1.2518\n",
      "Epoch: 49/100... Step: 18550... Loss: 1.0345... Val Loss: 1.2492\n",
      "Epoch: 49/100... Step: 18600... Loss: 1.0819... Val Loss: 1.2505\n",
      "Epoch: 49/100... Step: 18650... Loss: 1.0519... Val Loss: 1.2517\n",
      "Epoch: 49/100... Step: 18700... Loss: 1.0638... Val Loss: 1.2572\n",
      "Epoch: 50/100... Step: 18750... Loss: 1.0745... Val Loss: 1.2554\n",
      "Epoch: 50/100... Step: 18800... Loss: 1.1003... Val Loss: 1.2530\n",
      "Epoch: 50/100... Step: 18850... Loss: 1.0920... Val Loss: 1.2546\n",
      "Epoch: 50/100... Step: 18900... Loss: 1.0805... Val Loss: 1.2546\n",
      "Epoch: 50/100... Step: 18950... Loss: 1.0905... Val Loss: 1.2542\n",
      "Epoch: 50/100... Step: 19000... Loss: 1.0737... Val Loss: 1.2551\n",
      "Epoch: 50/100... Step: 19050... Loss: 1.0766... Val Loss: 1.2556\n",
      "Epoch: 50/100... Step: 19100... Loss: 1.1599... Val Loss: 1.2543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/100... Step: 19150... Loss: 1.0854... Val Loss: 1.2503\n",
      "Epoch: 51/100... Step: 19200... Loss: 1.0588... Val Loss: 1.2493\n",
      "Epoch: 51/100... Step: 19250... Loss: 1.0441... Val Loss: 1.2567\n",
      "Epoch: 51/100... Step: 19300... Loss: 1.0744... Val Loss: 1.2533\n",
      "Epoch: 51/100... Step: 19350... Loss: 1.1094... Val Loss: 1.2522\n",
      "Epoch: 51/100... Step: 19400... Loss: 1.0845... Val Loss: 1.2506\n",
      "Epoch: 51/100... Step: 19450... Loss: 1.0585... Val Loss: 1.2586\n",
      "Epoch: 52/100... Step: 19500... Loss: 1.0727... Val Loss: 1.2527\n",
      "Epoch: 52/100... Step: 19550... Loss: 1.1268... Val Loss: 1.2561\n",
      "Epoch: 52/100... Step: 19600... Loss: 1.0929... Val Loss: 1.2535\n",
      "Epoch: 52/100... Step: 19650... Loss: 1.1050... Val Loss: 1.2547\n",
      "Epoch: 52/100... Step: 19700... Loss: 1.0779... Val Loss: 1.2545\n",
      "Epoch: 52/100... Step: 19750... Loss: 1.0915... Val Loss: 1.2547\n",
      "Epoch: 52/100... Step: 19800... Loss: 1.1128... Val Loss: 1.2561\n",
      "Epoch: 52/100... Step: 19850... Loss: 1.0380... Val Loss: 1.2613\n",
      "Epoch: 53/100... Step: 19900... Loss: 1.0285... Val Loss: 1.2562\n",
      "Epoch: 53/100... Step: 19950... Loss: 1.0752... Val Loss: 1.2549\n",
      "Epoch: 53/100... Step: 20000... Loss: 1.0796... Val Loss: 1.2539\n",
      "Epoch: 53/100... Step: 20050... Loss: 1.0881... Val Loss: 1.2571\n",
      "Epoch: 53/100... Step: 20100... Loss: 1.0667... Val Loss: 1.2537\n",
      "Epoch: 53/100... Step: 20150... Loss: 1.1897... Val Loss: 1.3079\n",
      "Epoch: 53/100... Step: 20200... Loss: 1.1226... Val Loss: 1.2561\n",
      "Epoch: 54/100... Step: 20250... Loss: 1.0690... Val Loss: 1.2516\n",
      "Epoch: 54/100... Step: 20300... Loss: 1.0951... Val Loss: 1.2527\n",
      "Epoch: 54/100... Step: 20350... Loss: 1.0665... Val Loss: 1.2543\n",
      "Epoch: 54/100... Step: 20400... Loss: 1.0646... Val Loss: 1.2555\n",
      "Epoch: 54/100... Step: 20450... Loss: 1.0745... Val Loss: 1.2560\n",
      "Epoch: 54/100... Step: 20500... Loss: 1.1147... Val Loss: 1.2524\n",
      "Epoch: 54/100... Step: 20550... Loss: 1.0972... Val Loss: 1.2517\n",
      "Epoch: 54/100... Step: 20600... Loss: 1.1016... Val Loss: 1.2591\n",
      "Epoch: 55/100... Step: 20650... Loss: 1.0979... Val Loss: 1.2548\n",
      "Epoch: 55/100... Step: 20700... Loss: 1.0671... Val Loss: 1.2594\n",
      "Epoch: 55/100... Step: 20750... Loss: 1.0446... Val Loss: 1.2492\n",
      "Epoch: 55/100... Step: 20800... Loss: 1.0857... Val Loss: 1.2525\n",
      "Epoch: 55/100... Step: 20850... Loss: 1.0819... Val Loss: 1.2531\n",
      "Epoch: 55/100... Step: 20900... Loss: 1.0475... Val Loss: 1.2550\n",
      "Epoch: 55/100... Step: 20950... Loss: 1.0785... Val Loss: 1.2572\n",
      "Epoch: 55/100... Step: 21000... Loss: 1.1070... Val Loss: 1.2585\n",
      "Epoch: 56/100... Step: 21050... Loss: 1.0775... Val Loss: 1.2596\n",
      "Epoch: 56/100... Step: 21100... Loss: 1.0771... Val Loss: 1.2587\n",
      "Epoch: 56/100... Step: 21150... Loss: 1.0564... Val Loss: 1.2522\n",
      "Epoch: 56/100... Step: 21200... Loss: 1.0856... Val Loss: 1.2536\n",
      "Epoch: 56/100... Step: 21250... Loss: 1.0943... Val Loss: 1.2565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-18abb09d5203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-c130d27c1815>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchenv_gpu\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mtotal_norm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "              \n",
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_name = 'shake_50_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_name = 'shake_50_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Defining a method to generate the next character\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALD,                               Iago, with trumpets.\n",
      "                                     [They set them stroke and]\n",
      "\n",
      "          Enter BOLINGBROKE, and other LADIES and SOLDIERS\n",
      "\n",
      "    And, when I humbly warrant thee, sweet soldier,  \n",
      "    And we are set this means to stand and beat,\n",
      "    As thus I spill, and stop as true; worse, honest truth;\n",
      "    By him that I might hate himself in thee,\n",
      "    As I am all our crown against the spirits\n",
      "    And speaked against them. I shall see at home,\n",
      "    Are bound to meet the power to say to me.\n",
      "    And, for the wild and that, are not to send\n",
      "    The thick as there. Have you a subject time\n",
      "    The conference which I would be an old measure?\n",
      "    And to be made as madam to my brain.\n",
      "    Who should steal her and more? To the cheer of men,\n",
      "    This is a sport that sent you on the way\n",
      "    To be thy father to thy lovely blessings;\n",
      "    And, by those what should be my pow'ring souls,\n",
      "    I warrant thee, sir, that was not but a man\n",
      "    To many a senanor of my power\n"
     ]
    }
   ],
   "source": [
    "# Declaring a method to generate new text\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "    \n",
    "# Generating new text\n",
    "print(sample(net, 1000, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_ch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1c0243c5d231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtop_ch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'top_ch' is not defined"
     ]
    }
   ],
   "source": [
    "top_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
