{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pierian-Data-Logo.PNG\">\n",
    "<br>\n",
    "<strong><center>Copyright 2019. Created by Jose Marcial Portilla.</center></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply an RNN to Text\n",
    "Given their ability to remember past patterns, RNNs are good at predicting what character should follow a given sequence of characters, or what word might complete a given phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "For this exercise we'll use <em>The Adventures of Tom Sawyer</em> by Mark Twain, available from Project Gutenberg at http://www.gutenberg.org/ebooks/74.<br>\n",
    "We've removed most of the front matter (Table of Contents, List of Illustrations, etc.) and retained just the chapter headings and the text of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/TomSawyer.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE ADVENTURES OF TOM SAWYER\\n\\nBy Mark Twain\\n(Samuel Langhorne Clemens)\\n\\nPREFACE\\n\\nMost of the adventures recorded in this book really occurred; one or two\\nwere experiences of my own, the rest those of boys who were schoolmates\\nof mine. Huck Finn is drawn from life; Tom Sawyer also, but not from an\\nindividual--he is a combination of the characteristics of three boys whom\\nI knew, and therefore belongs to the composite order of architecture.\\n\\nThe odd superstitions touched upon were all prevalent among children and\\nslaves in the West at the period of this story--that is to say, thirty or\\nforty years ago.\\n\\nAlthough my book is intended mainly for the entertainment of boys and\\ngirls, I hope it will not be shunned by men and women on that account,\\nfor part of my plan has been to try to pleasantly remind adults of what\\nthey once were themselves, and of how they felt and thought and talked,\\nand what queer enterprises they sometimes engaged in.\\n\\nTHE AUTHOR.\\n\\nHARTFORD, 1876.\\n\\n\\nCHAPTER I\\n\\n“TOM!”\\n\\nNo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <tt><strong>\\n</strong></tt> is a Python <a href='https://docs.python.org/3/reference/lexical_analysis.html'>escape sequence</a> that represents a line feed, and it counts as one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R\\n\\nB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display four connected characters:\n",
    "text[27:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the characters\n",
    "We want to first identify all the unique characters, including digits and punctuation, contained in the test. We can do this quickly and efficiently by casting the text as a set. Then we want to assign integers to each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B', 'v', 'r', 's', 'R', 'M', 'g', 'n', '3', '8', 'L', 'u', '$', 'Y', 'D', 'W', '@', 'i', '\\n', 'P', 'l', 'H', 'z', 'p', 'q', 'K', 'o', 'V', '(', '_', ']', 'b', ':', '.', '4', '&', '*', 'I', 'y', '2', '7', 'j', 'E', '“', '%', 'N', 'h', 'F', \"'\", 'A', 'd', 'O', '[', '?', ';', 'J', 'a', '1', 'Q', '0', 'e', 'f', 't', '5', ' ', 'w', 'T', 'X', 'x', '”', 'G', 'S', 'C', '!', '9', '/', 'm', 'c', 'k', '6', ')', ',', '-', 'U'}\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "char_list = sorted(list(chars))\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to assign a unique integer to each character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '@': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, '[': 53, ']': 54, '_': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '“': 82, '”': 83}\n"
     ]
    }
   ],
   "source": [
    "encoder = {}\n",
    "for i,x in enumerate(char_list):\n",
    "    encoder[x] = i\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an encoder, we can encode the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = [encoder[x] for x in text]\n",
    "encoded_text = torch.LongTensor(encoded_text)\n",
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF TOM SAWYER\n",
      "tensor([47, 35, 32,  1, 28, 31, 49, 32, 41, 47, 48, 45, 32, 46,  1, 42, 33,  1,\n",
      "        47, 42, 40,  1, 46, 28, 50, 52, 32, 45])\n"
     ]
    }
   ],
   "source": [
    "print(text[:28])\n",
    "print(encoded_text[:28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training data\n",
    "We want to use the entire corpus for training. Since we're using an LSTM, we want to created a window of one-hot-encoded characters followed by an integer label. In this way the argmax of the prediction should match the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = encoded_text[:-1].view(len(encoded_text)-1,1).type(torch.float)\n",
    "# y_train = encoded_text[1:].type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(seq,ws):  # ws is the window size\n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = input_data(encoded_text,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([47, 35, 32,  1, 28, 31, 49, 32, 41, 47, 48, 45, 32, 46,  1, 42, 33,  1,\n",
       "         47, 42]), tensor([40]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "Our input size is going to be 1, the number of hidden layers is arbitrary - we'll use 128. The output size is 84 and we'll use CrossEntropyLoss as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, out_size=84):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Add an LSTM layer:\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size)\n",
    "        \n",
    "        # Add a fully-connected layer:\n",
    "        self.linear = nn.Linear(hidden_size,out_size)\n",
    "        \n",
    "        # Initialize h0 and c0:\n",
    "        self.hidden = (torch.zeros(1,1,hidden_size),torch.zeros(1,1,hidden_size))\n",
    "    \n",
    "    def forward(self,seq):\n",
    "        lstm_out, self.hidden = self.lstm(seq.view(len(seq), 1, -1), self.hidden)\n",
    "        pred = self.linear(lstm_out.view(len(seq),-1))\n",
    "        return pred[-1]   # we only care about the last prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model, define loss & optimization functions\n",
    "Since we're running a classification, we'll use <a href='https://pytorch.org/docs/stable/nn.html#crossentropyloss'><tt><strong>torch.nn.CrossEntropyLoss</strong></tt></a><br>Also, we've found that <a href='https://pytorch.org/docs/stable/optim.html#torch.optim.SGD'><tt><strong>torch.optim.SGD</strong></tt></a> converges faster for this application than <a href='https://pytorch.org/docs/stable/optim.html#torch.optim.Adam'><tt><strong>torch.optim.Adam</strong></tt></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 128)\n",
       "  (linear): Linear(in_features=128, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "model = LSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   512\n",
      " 65536\n",
      "   512\n",
      "   512\n",
      " 10752\n",
      "    84\n",
      "______\n",
      " 77908\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: LOAD OUR MODEL\n",
    "\n",
    "**Training this takes a very, very long time! You may just want to load our provided model here and skip the training, training takes AT LEAST an hour on a fast computer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 128)\n",
       "  (linear): Linear(in_features=128, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = LSTM()\n",
    "saved_model.load_state_dict(torch.load('MarkTwainModel2.pt'));\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "Epoch:  1 Loss: 1.92200708\n",
      "\n",
      "Duration: 3395 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch = 0\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the entire set of data\n",
    "    for seq, y_train in train_data[178330:]:  \n",
    "        seq = seq.type(torch.float).view(-1,1).cuda()\n",
    "        y_train = y_train.view(-1).cuda()\n",
    "        \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq).view(1,-1) # this wants to be 2D\n",
    "        \n",
    "        loss = criterion(y_pred,y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch+=1\n",
    "        if batch%10000 == 0:\n",
    "            print(batch)\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227920"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batches processed after 1 hr out of 406250:\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "We'll save this in a file called \"MarkTwainModel.pt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MarkTwainModel_overwrite.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([76, 69, 59, 56, 75, 64, 70, 69, 10,  1, 63, 70, 78,  1, 75, 70,  1, 63,\n",
       "         60, 67]), tensor([71]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "# Generating New Text\n",
    "----\n",
    "----\n",
    "\n",
    "## Step 1: Create Seed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"part of my plan has been to try\"[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seed_text) == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Encode Seed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(seed_text):\n",
    "    encoded_char_list = []\n",
    "    for character in seed_text:\n",
    "        encoded_char_list.append(encoder[character])\n",
    "    \n",
    "    return torch.Tensor(np.array(encoded_char_list)).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_tensor = encode_text(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[71.],\n",
       "        [56.],\n",
       "        [73.],\n",
       "        [75.],\n",
       "        [ 1.],\n",
       "        [70.],\n",
       "        [61.],\n",
       "        [ 1.],\n",
       "        [68.],\n",
       "        [80.],\n",
       "        [ 1.],\n",
       "        [71.],\n",
       "        [67.],\n",
       "        [56.],\n",
       "        [69.],\n",
       "        [ 1.],\n",
       "        [63.],\n",
       "        [56.],\n",
       "        [74.],\n",
       "        [ 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Next Predicted Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = {v: k for k, v in encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(seq,model):\n",
    "    pred_tensor = model(seq)\n",
    "    max_prob_char = pred_tensor.argmax().item()\n",
    "    pred_char = max_prob_char\n",
    "    return decoder[pred_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char(encoded_text_tensor,saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Loop for N predicted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_full_text = seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of my plan has '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of my plan has ooo ooo ooo ooo ooo ooo ooo ooo ooo ooo \n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "for i in range(n):\n",
    "    \n",
    "    last_20_char = encode_text(my_full_text[-20:])\n",
    "    pred_char = predict_next_char(last_20_char,saved_model) \n",
    "    my_full_text += pred_char\n",
    "    \n",
    "print(my_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
