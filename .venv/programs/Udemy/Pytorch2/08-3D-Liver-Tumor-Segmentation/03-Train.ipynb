{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broken-indonesia",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will train the 3D Unet to segment the liver and liver tumors in CT scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-essay",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* HTML for visualizing volume videos\n",
    "* torchio for dataset creation\n",
    "* torch for DataLoaders, optimizer and loss\n",
    "* pytorch-lightning for training\n",
    "* numpy for masking\n",
    "* matplotlib for visualization\n",
    "* Our 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-canon",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "We can loop over all available scans and add them to the subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_to_label_path(path):\n",
    "    \"\"\"\n",
    "    Replace data with mask to get the masks\n",
    "    \"\"\"\n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
    "    return Path(*parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-plastic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = Path(\"Task03_Liver_rs/imagesTr/\")\n",
    "subjects_paths = list(path.glob(\"liver_*\"))\n",
    "subjects = []\n",
    "\n",
    "for subject_path in subjects_paths:\n",
    "    label_path = change_img_to_label_path(subject_path)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(subject_path), \"Label\":tio.LabelMap(label_path)})\n",
    "    subjects.append(subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    assert subject[\"CT\"].orientation == (\"R\", \"A\", \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-bread",
   "metadata": {},
   "source": [
    "We use the same  augmentation steps as used in the Dataset notebook. <br />\n",
    "Regarding the processing, we use the **CropOrPad** functionality which crops or pads all images and masks to the same shape. <br />\n",
    "\n",
    "We use ($256 \\times 256 \\times 200$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = tio.Compose([\n",
    "            tio.CropOrPad((256, 256, 200)),\n",
    "            tio.RescaleIntensity((-1, 1))\n",
    "            ])\n",
    "\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "\n",
    "val_transform = process\n",
    "train_transform = tio.Compose([process, augmentation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-construction",
   "metadata": {},
   "source": [
    "Define the train and validation dataset. We use 105 subjects for training and 13 for testing. <br />\n",
    "In order to help the segmentation network learn, we use the LabelSampler with p=0.2 for background, p=0.3 for liver and p=0.5 for liver tumors with a patch size of ($96 \\times 96 \\times 96$).\n",
    "\n",
    "Feel free to try the UniformSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(subjects[:105], transform=train_transform)\n",
    "val_dataset = tio.SubjectsDataset(subjects[105:], transform=val_transform)\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size=96, label_name=\"Label\", label_probabilities={0:0.2, 1:0.3, 2:0.5})\n",
    "#sampler = tio.data.UniformSampler(patch_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-smile",
   "metadata": {},
   "source": [
    "Create the queue to draw patches from.<br />\n",
    "The tio.Queue accepts a SubjectsDataset, a max_length argument describing the the number of patches that can be stored, the number of patches to draw from each subject, a sampler and the number of workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Adapt max_length and num_workers to your hardware\n",
    "\n",
    "train_patches_queue = tio.Queue(\n",
    "     train_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "     val_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-finding",
   "metadata": {},
   "source": [
    "Define train and val loader:\n",
    "\n",
    "NOTE: As the dataloaders only pop patches from the queue use 0 num workers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, adapt batch size according to your hardware\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_patches_queue, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_patches_queue, batch_size=batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-midwest",
   "metadata": {},
   "source": [
    "Finally we can create the Segmentation model.\n",
    "\n",
    "We use the Adam optimizer with a learning rate of 1e-4 and a weighted cross-entropy loss, which assigns a threefold increased loss to tumorous voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def log_images(self, img, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        pred = torch.argmax(pred, 1) # Take the output with the highest value\n",
    "        axial_slice = 50  # Always plot slice 50 of the 96 slices\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][:,:,axial_slice]==0, mask[0][:,:,axial_slice])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][:,:,axial_slice]==0, pred[0][:,:,axial_slice])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "gpus = 1 #TODO\n",
    "trainer = pl.Trainer(gpus=gpus, logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-superintendent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "# This might take some hours depending on your GPU\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-favorite",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from celluloid import Camera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-memphis",
   "metadata": {},
   "source": [
    "First we load the model and place it on the gpu if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-passage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Segmenter.load_from_checkpoint(\"weights/epoch=97-step=25773.ckpt\")\n",
    "model = model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-korea",
   "metadata": {},
   "source": [
    "### Patch Aggregation\n",
    "The model was trained in a patch wise manner as the full volumes are too large to be placed on a typical GPU.\n",
    "But we still want to get a result for the whole volume.<br />\n",
    "torchio helps us doing so by performing *Patch Aggregation*\n",
    "\n",
    "The goal of patch aggregation is to split the image into patches, then compute the segmentation for each patch and finally merge the predictions into the prediction for the full volume.\n",
    "\n",
    "The pipeline is as follows:\n",
    "1. Define the **GridSampler(subject, patch_size, patch_overlap)** responsible for dividing the volume into patches. Each patch is defined by its location accesible via *tio.LOCATION*\n",
    "2. Define the **GridAggregator(grid_sampler)** which merges the predicted patches back together\n",
    "3. Compute the prediction on the patches and aggregate them via **aggregator.add_batch(pred, location)**\n",
    "4. Extract the full prediction via **aggregator.get_output_tensor()**\n",
    "\n",
    "Additionally, we can leverage the DataLoader from pytorch to perform the prediction in a batch wise manner for a nice speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a validation subject and extract the images and segmentation for evaluation\n",
    "IDX = 4\n",
    "mask = val_dataset[IDX][\"Label\"][\"data\"]\n",
    "imgs = val_dataset[IDX][\"CT\"][\"data\"]\n",
    "\n",
    "# GridSampler\n",
    "grid_sampler = tio.inference.GridSampler(val_dataset[IDX], 96, (8, 8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridAggregator\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for speed up\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['CT'][\"data\"].to(device)  # Get batch of patches\n",
    "        locations = patches_batch[tio.LOCATION]  # Get locations of patches\n",
    "        pred = model(input_tensor)  # Compute prediction\n",
    "        aggregator.add_batch(pred, locations)  # Combine predictions to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the volume prediction\n",
    "output_tensor = aggregator.get_output_tensor()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-planner",
   "metadata": {},
   "source": [
    "Finally we can visualize the prediction as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "pred = output_tensor.argmax(0)\n",
    "\n",
    "for i in range(0, output_tensor.shape[3], 2):  # axial view\n",
    "    plt.imshow(imgs[0,:,:,i], cmap=\"bone\")\n",
    "    mask_ = np.ma.masked_where(pred[:,:,i]==0, pred[:,:,i])\n",
    "    label_mask = np.ma.masked_where(mask[0,:,:,i]==0, mask[0,:,:,i])\n",
    "    plt.imshow(mask_, alpha=0.1, cmap=\"autumn\")\n",
    "    #plt.imshow(label_mask, alpha=0.5, cmap=\"jet\")  # Uncomment if you want to see the label\n",
    "\n",
    "    # plt.axis(\"off\")\n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animation.to_html5_video())  # convert the animation to a video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
