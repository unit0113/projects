{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-relative",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Finally you are going to train our tumor segmentation network. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-symphony",
   "metadata": {},
   "source": [
    "## Imports\n",
    "**Task: Import the necessary libraries** <br />\n",
    "Hint: Make sure that you copy Dataset and model to separate python files so you can import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-salem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "conscious-croatia",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "**Task: Create the train and val dataset and the augmentation pipeline. Use Affine augmentations with**:\n",
    "1. 15% translation,\n",
    "2. scaling between 0.85 and 1.15\n",
    "3. rotations from -45 to 45Â°.\n",
    "\n",
    "Additionally use **ElasticTransformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-marine",
   "metadata": {},
   "source": [
    "## Oversampling to tackle strong class imbalance\n",
    "Lung tumors are often very small, thus we need to make sure that our model does not learn a trivial solution which simply outputs 0 for all voxels.<br />\n",
    "In this notebook we will use oversampling to sample slices which contain a tumor more often.\n",
    "\n",
    "To do so we can use the **WeightedRandomSampler** provided by pytorch which needs a weight for each sample in the dataset.\n",
    "Typically you have one weight for each class, which means that we need to calculate two weights, one for slices without tumors and one for slices with a tumor and create list that assigns each sample from the dataset the corresponding weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-neighborhood",
   "metadata": {},
   "source": [
    "To do so, we at first need to create a list containing only the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for _, label in tqdm(train_dataset):\n",
    "    # Check if mask contains a tumorous pixel:\n",
    "    if np.any(label):\n",
    "        target_list.append(1)\n",
    "    else:\n",
    "        target_list.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-partition",
   "metadata": {},
   "source": [
    "Then we need to calculate the weight for each class:\n",
    "To do so, we can simply compute the fraction between the classes and then create the weight list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = np.unique(target_list, return_counts=True)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = uniques[1][0] / uniques[1][1]\n",
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-yukon",
   "metadata": {},
   "source": [
    "Subsequently we assign the weight 1 to each slice without a tumor and ~ 9 to each slice with a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "for target in target_list:\n",
    "    if target == 0:\n",
    "        weight_list.append(1)\n",
    "    else:\n",
    "        weight_list.append(fraction)\n",
    "weight_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-webmaster",
   "metadata": {},
   "source": [
    "Finally we create the sampler which we can pass to the DataLoader.\n",
    "**Important:** Only use a sampler for the train loader! We don't want to change the validation data to get a real validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_list, len(weight_list))                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-uganda",
   "metadata": {},
   "source": [
    "**Task: Create the train and val_loaders. Set batch size and num workers according to your hardware. Use the sampler for the train_loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-prefix",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-throw",
   "metadata": {},
   "source": [
    "We can verify that our sampler works by taking a batch from the train loader and count how many labels are larger than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sampler = next(iter(train_loader))  # Take one batch\n",
    "(verify_sampler[1][:,0]).sum([1, 2]) > 0   # ~ half the batch size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-stanley",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "As this is a harder task to train you might try different loss functions:\n",
    "We achieved best results by using the Binary Cross Entropy instead of the Dice Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-clone",
   "metadata": {},
   "source": [
    "## Full Segmentation Model\n",
    "\n",
    "**Task: Create the pytorch lightning model. Use Binary Cross Entropy as loss function and the Adam optimizer with a learning rate of 1e-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rolled-cardiff",
   "metadata": {},
   "source": [
    "**Task: Instanciate the model, create a checkpoint callback and define the trainer.<br /> Train the model for 30 epochs and use a TensorboardLogger to log your training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-macintosh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-animal",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "**Task: Load the latest checkpoint and evaluate the results by computing the prediction for the complete validation dataset and then compute the dice score for it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-pendant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "christian-municipality",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "**Task: Compute a prediction for a patient and visualize the prediction.**<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aware-convert",
   "metadata": {},
   "source": [
    "Congratulations! You just built a lung cancer segmentation model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Udemy",
   "language": "python",
   "name": "udemy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
