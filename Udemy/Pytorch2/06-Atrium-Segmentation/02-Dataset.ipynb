{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laughing-palace",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will create the dataset class used to feed slices and corresponding segmentation masks to the network during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-lawrence",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* torch for dataset creation\n",
    "* numpy for file loading and processing\n",
    "* imgaug to set the random seed\n",
    "* SegmentationMapsOnImage from imgaug to augment the segmentation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-tractor",
   "metadata": {},
   "source": [
    "## DataSet Creation\n",
    "We need to implement the following functionality:\n",
    "1. Create a list of all 2D slices. To do so we need to extract all slices from all subjects\n",
    "2. Extract the corresponding label path for each slice path\n",
    "3. Load slice and label\n",
    "4. Data Augmentation. Make sure that slice and mask are augmented identically. imgaug handles this for us, thus we will not use torchvision.transforms for that\n",
    "5. Return slice and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, augment_params):\n",
    "        self.all_files = self.extract_files(root)\n",
    "        self.augment_params = augment_params\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_files(root):\n",
    "        \"\"\"\n",
    "        Extract the paths to all slices given the root path (ends with train or val)\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for subject in root.glob(\"*\"):   # Iterate over the subjects\n",
    "            slice_path = subject/\"data\"  # Get the slices for current subject\n",
    "            for slice in slice_path.glob(\"*.npy\"):\n",
    "                files.append(slice)\n",
    "        return files\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def change_img_to_label_path(path):\n",
    "        \"\"\"\n",
    "        Replace data with mask to get the masks\n",
    "        \"\"\"\n",
    "        parts = list(path.parts)\n",
    "        parts[parts.index(\"data\")] = \"masks\"\n",
    "        return Path(*parts)\n",
    "\n",
    "    def augment(self, slice, mask):\n",
    "        \"\"\"\n",
    "        Augments slice and segmentation mask in the exact same way\n",
    "        Note the manual seed initialization\n",
    "        \"\"\"\n",
    "        ###################IMPORTANT###################\n",
    "        # Fix for https://discuss.pytorch.org/t/dataloader-workers-generate-the-same-random-augmentations/28830/2\n",
    "        random_seed = torch.randint(0, 1000000, (1,)).item()\n",
    "        imgaug.seed(random_seed)\n",
    "        #####################################################\n",
    "        mask = SegmentationMapsOnImage(mask, mask.shape)\n",
    "        slice_aug, mask_aug = self.augment_params(image=slice, segmentation_maps=mask)\n",
    "        mask_aug = mask_aug.get_arr()\n",
    "        return slice_aug, mask_aug\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset (length of all files)\n",
    "        \"\"\"\n",
    "        return len(self.all_files)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given an index return the (augmented) slice and corresponding mask\n",
    "        Add another dimension for pytorch\n",
    "        \"\"\"\n",
    "        file_path = self.all_files[idx]\n",
    "        mask_path = self.change_img_to_label_path(file_path)\n",
    "        slice = np.load(file_path).astype(np.float32)  # Convert to float for torch\n",
    "        mask = np.load(mask_path)\n",
    "        \n",
    "        if self.augment_params:\n",
    "            slice, mask = self.augment(slice, mask)\n",
    "        \n",
    "        # Note that pytorch expects the input of shape BxCxHxW, where B corresponds to the batch size, C to the channels, H to the height and W to Width.\n",
    "        # As our data is of shape (HxW) we need to manually add the C axis by using expand_dims.\n",
    "        # The batch dimension is later added by the dataloader\n",
    "\n",
    "        return np.expand_dims(slice, 0), np.expand_dims(mask, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-wallpaper",
   "metadata": {},
   "source": [
    "Now we can test our dataset!\n",
    "Let us at first define the data augmentation routine responsible for scaling and rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imgaug.augmenters as iaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(scale=(0.85, 1.15), # Zoom in or out\n",
    "               rotate=(-45, 45)),  # Rotate up to 45 degrees\n",
    "    iaa.ElasticTransformation()  # Random Elastic Deformations\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object\n",
    "path = Path(\"Preprocessed/train/\")\n",
    "dataset = CardiacDataset(path, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-ensemble",
   "metadata": {},
   "source": [
    "Finally, we can visualize our dataset.\n",
    "To make sure that the augmentation works properly, we access the same dataset element multiple times and visualize the augmented images and masks.\n",
    "\n",
    "Please chose an index where the left atrium is visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        slice, mask = dataset[3]\n",
    "        mask_ = np.ma.masked_where(mask==0, mask)\n",
    "        axis[i][j].imshow(slice[0], cmap=\"bone\")\n",
    "        axis[i][j].imshow(mask_[0], cmap=\"autumn\")\n",
    "        axis[i][j].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Sample augmentations\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-affair",
   "metadata": {},
   "source": [
    "Nice!\n",
    "With above dataset we can finally create the model and train the AtriumSegmenter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
