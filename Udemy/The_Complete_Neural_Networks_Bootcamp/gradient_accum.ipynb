{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNkm02TL0wX_"
   },
   "source": [
    "### Gradient Accumulation\n",
    "\n",
    "- In many situations, we want to have a high batch size (desired batch size), however our GPU can only handle a specific batch size (tolerable batch size). One option is to have multiple GPUs and use distributed data training. But what if only one GPU is available? The solution is **gradient accumulation**.\n",
    "<br>\n",
    "<br>\n",
    "- Gradient accumulation (summation) is performing **multiple** backwards passes **before** updating the parameters. The goal is to have the same model parameters for multiple inputs (batches) and then update the model's parameters based on all these batches, instead of performing an update after every single batch.  So we run each torelarbale batch size individually with the same model parameters and calculate the gradients without updating the model. When the desired batch size is reached, we can then update the gradients. \n",
    "<br>\n",
    "<br>\n",
    "- Point of confusion for sudents. The **computational graph** is automatically destroyed when .backward() is called (unless retain_graph=True is specified), and **NOT** the gradients. The gradients are only reset when calling optimizer.zero_grad()\n",
    "<br>\n",
    "<br>\n",
    "- Let's implement that on a ResNet-101 using Google Colab GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SZFbPs20_3j"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet101()\n",
    "num_iterations = 10\n",
    "xe = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfTbnNVL1Odo"
   },
   "outputs": [],
   "source": [
    "batch_size = 50   # this works, 100 does not\n",
    "for i in range(num_iterations):\n",
    "    inputs = torch.randn(batch_size,3,224,224)\n",
    "    labels = torch.LongTensor(batch_size).random_(0, 100)\n",
    "    loss = xe(model(inputs), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Done one batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBtqQLBx4hzh"
   },
   "outputs": [],
   "source": [
    "desired_batch_size = 100\n",
    "tolerable_batch_size = 50\n",
    "accum_steps = desired_batch_size / tolerable_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plu7srYS1cTl"
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    inputs = torch.randn(tolerable_batch_size,3,224,224)\n",
    "    labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n",
    "    loss = xe(model(inputs), labels)\n",
    "    # The loss needs to be scaled to have the same significance over the whole dataset, because the mean should be taken across\n",
    "    # the whole dataset , which requires the loss to be divided by the number of batches. This is only the case if the loss \n",
    "    # returned is averaged. But if the loss returned is summed (for example: nn.BCELoss(reduction = 'sum')) then we do not need to do this\n",
    "    loss = loss / accum_steps  \n",
    "    loss.backward()\n",
    "\n",
    "    if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Done one batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_7eQqM22Af4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gradient accum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
