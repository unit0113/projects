{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_moons(200, noise = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 2\n",
    "output_neurons = 2\n",
    "samples = X.shape[0]\n",
    "learning_rate = 0.001\n",
    "lambda_reg = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive(model_dict):\n",
    "    W1 = model_dict['W1']\n",
    "    b1 = model_dict['b1']\n",
    "    W2 = model_dict['W2']\n",
    "    b2 = model_dict['b2']\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, model_dict):\n",
    "    W1, b1, W2, b2 = retreive(model_dict)\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True) \n",
    "    return z1, a1, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(softmax, y, model_dict):\n",
    "    W1, b1, W2, b2 = retreive(model_dict)\n",
    "    m = np.zeros(200)\n",
    "    for i,correct_index in enumerate(y):\n",
    "        predicted = softmax[i][correct_index]\n",
    "        m[i] = predicted\n",
    "    log_prob = -np.log(m)\n",
    "    loss = np.sum(log_prob)\n",
    "    reg_loss = lambda_reg / 2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    loss+= reg_loss\n",
    "    return float(loss / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_dict, x):\n",
    "    W1, b1, W2, b2 = retreive(model_dict)\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True)   # (200,2)\n",
    "    return np.argmax(softmax, axis = 1)    # (200,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations for Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](https://user-images.githubusercontent.com/30661597/67566804-3e405980-f75b-11e9-9e19-f3c388745c0a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial y_{1}}=-\\tilde{y}_{1}\\left[\\frac{\\left(y_{1}\\right)^{\\prime}}{y_{1}}\\right]=-\\tilde{y}_{1}\\left[\\frac{1}{y_{1}}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial y_{1}}{\\partial z_{1}}=\\frac{\\partial}{\\partial z_{1}} \\cdot \\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![der](https://user-images.githubusercontent.com/46392773/56460220-ec94cc00-63d1-11e9-91d1-b4e3c57c81fd.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial z_{1}}=-\\tilde{y}_{1}\\left[\\frac{1}{y_{1}}\\right]\\frac{\\partial y_{1}}{\\partial z_{1}}=-\\tilde{y}_{1}\\left[\\frac{y_{1}\\left(1-y_{1}\\right)}{y_{1}}\\right]=-\\tilde{y}_{1}\\left(1-y_{1}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our labels are in the form of a one-hot vector: (ex. If class 1 is correct, then the corresponding label is [1,0], if class 2 is correct, then the corresponding label is [0,1]).\n",
    "\n",
    "If class one is the actual prediction: $\\tilde{y}_{1} = 1 $, then we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial z_{1}} = -1(1-y_{1}) = y_{1} - 1 = \\delta_{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\delta_{2}=\\left(1-\\tanh ^{2} z_{1}\\right) \\circ \\delta_{3} W_{2}^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial W_{2}}=a_{1}^{T} \\delta_{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial b_{2}}=\\delta_{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial W_{1}}=x^{T} \\delta 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial b_{1}}=\\delta 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x, y, model_dict, epochs):\n",
    "    for i in range(epochs):\n",
    "        W1, b1, W2, b2 = retreive(model_dict)\n",
    "        z1, a1, probs = forward(x, model_dict)    # a1: (200,3), probs: (200,2)\n",
    "        delta3 = np.copy(probs)\n",
    "        delta3[range(x.shape[0]), y] -= 1      # (200,2)\n",
    "        dW2 = (a1.T).dot(delta3)               # (3,2)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)        # (1,2)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(np.tanh(z1), 2))\n",
    "        dW1 = np.dot(x.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        # Add regularization terms\n",
    "        dW2 += lambda_reg * np.sum(W2)  \n",
    "        dW1 += lambda_reg * np.sum(W1)  \n",
    "        # Update Weights: W = W + (-lr*gradient) = W - lr*gradient\n",
    "        W1 += -learning_rate * dW1\n",
    "        b1 += -learning_rate * db1\n",
    "        W2 += -learning_rate * dW2\n",
    "        b2 += -learning_rate * db2\n",
    "        # Update the model dictionary\n",
    "        model_dict = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        # Print the loss every 50 epochs\n",
    "        if i%50 == 0:\n",
    "            print(\"Loss at epoch {} is: {:.3f}\".format(i,loss(probs, y, model_dict)))\n",
    "            \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Initial Weights\n",
    "def init_network(input_dim, hidden_dim, output_dim):\n",
    "    model = {}\n",
    "    # Xavier Initialization \n",
    "    W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "    model['W1'] = W1\n",
    "    model['b1'] = b1\n",
    "    model['W2'] = W2\n",
    "    model['b2'] = b2\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func): \n",
    "    \"\"\"\n",
    "    Code adopted from: https://github.com/dennybritz/nn-from-scratch\n",
    "    \"\"\"\n",
    "    # Set min and max values and give it some padding \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 \n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 \n",
    "    h = 0.01 \n",
    "    # Generate a grid of points with distance h between them \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) \n",
    "    # Predict the function value for the whole gid \n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()]) \n",
    "    Z = Z.reshape(xx.shape) \n",
    "    # Plot the contour and training examples \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) \n",
    "    plt.title(\"Decision Boundary for hidden layer size 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Let's start the action\n",
    "model_dict = init_network(input_dim = input_neurons , hidden_dim = 3, output_dim = output_neurons)\n",
    "model = backpropagation(X, y, model_dict, 1000)\n",
    "plot_decision_boundary(lambda x: predict(model, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
