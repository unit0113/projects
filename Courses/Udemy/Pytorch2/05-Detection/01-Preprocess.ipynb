{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "external-warren",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will preprocess the data for the cardiac detection task.\n",
    "We provide bounding boxes for around 500 images of the RSNA pneumonia detection challenge dataset which you have already downloaded in the last section. \n",
    "\n",
    "We will again convert the images to npy files for efficient storage and faster data loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-expansion",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* pydicom for reading dicom files\n",
    "* numpy for storing the actual images\n",
    "* cv2 for directly resizing the images\n",
    "* pandas to read the provided labels\n",
    "* matplotlib for visualization\n",
    "* patches from matplotlib to draw bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-mambo",
   "metadata": {},
   "source": [
    "At first, we read the csv file containing the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"./rsna_heart_detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path(\"/path/to/rsna-pneumonia-detection-challenge/stage_2_train_images/\", )\n",
    "SAVE_PATH = Path(\"Processed-Heart-Detection/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-selection",
   "metadata": {},
   "source": [
    "Let's visualize some images with corresponding bounding boxes around the heart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(2, 2)\n",
    "c = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        data = labels.iloc[c]\n",
    "        patient_id = data[\"name\"]\n",
    "        dcm_path = ROOT_PATH/str(patient_id)\n",
    "        dcm_path = dcm_path.with_suffix(\".dcm\")\n",
    "        \n",
    "        dcm = pydicom.read_file(dcm_path)\n",
    "        dcm_array = dcm.pixel_array\n",
    "        dcm_array = cv2.resize(dcm_array, (224, 224))\n",
    "        \n",
    "        x = data[\"x0\"]\n",
    "        y = data[\"y0\"]\n",
    "        width = data[\"w\"]\n",
    "        height = data[\"h\"]\n",
    "        \n",
    "        axis[i][j].imshow(dcm_array, cmap=\"bone\")\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor=\"r\", facecolor='none')\n",
    "        axis[i][j].add_patch(rect)\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-efficiency",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(2, 2)\n",
    "c = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        data = labels.iloc[c]  # Get the c-th row of the labels\n",
    "        \n",
    "        patient_id = data[\"name\"]\n",
    "        dcm_path = ROOT_PATH/str(patient_id)  # Create the path to the dcm file\n",
    "        dcm_path = dcm_path.with_suffix(\".dcm\")  # And add the .dcm suffix\n",
    "        dcm = pydicom.read_file(dcm_path)  # Read the dicom file with pydicom\n",
    "        \n",
    "        # Retrieve the actual image and resize it to match the labels\n",
    "        dcm_array = dcm.pixel_array\n",
    "        dcm_array = cv2.resize(dcm_array, (224, 224))\n",
    "        \n",
    "        x = data[\"x0\"]\n",
    "        y = data[\"y0\"]\n",
    "        width = data[\"w\"]\n",
    "        height = data[\"h\"]\n",
    "        \n",
    "        axis[i][j].imshow(dcm_array, cmap=\"bone\")\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        axis[i][j].add_patch(rect)\n",
    "\n",
    "        c+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-chancellor",
   "metadata": {},
   "source": [
    "We use a similar preprocessing routine to the one used for the classification task.<br />\n",
    "To be able to distinguish between train and validation subjects, we store them in two lists and later save these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = 0\n",
    "sums_squared = 0\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "\n",
    "for counter, patient_id in enumerate(list(labels.name)):  \n",
    "    dcm_path = ROOT_PATH/patient_id  # Create the path to the dcm file\n",
    "    dcm_path = dcm_path.with_suffix(\".dcm\")  # And add the .dcm suffix\n",
    "    \n",
    "    dcm = pydicom.read_file(dcm_path)  # Read the dicom file with pydicom\n",
    "    \n",
    "     # Retrieve the actual image \n",
    "    dcm_array = dcm.pixel_array\n",
    "    assert dcm_array.shape == (1024, 1024)\n",
    "    \n",
    "    # Resize the image to drastically improve training speed\n",
    "    # In order to reduce the space when storing the image we convert it to float16\n",
    "    # Standardize to 0-1 range\n",
    "    dcm_array = (cv2.resize(dcm_array, (224, 224)) / 255).astype(np.float16)\n",
    "            \n",
    "    # 4/5 train split, 1/5 val split\n",
    "    train_or_val = \"train\" if counter < 400 else \"val\" \n",
    "    \n",
    "    # Add to corresponding train or validation patient index list\n",
    "    if train_or_val == \"train\":\n",
    "        train_ids.append(patient_id)\n",
    "    else:\n",
    "        val_ids.append(patient_id)\n",
    "    \n",
    "    current_save_path = SAVE_PATH/train_or_val # Define save path and create if necessary\n",
    "    current_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(current_save_path/patient_id, dcm_array)  # Save the array in the corresponding directory\n",
    "    \n",
    "    normalizer = dcm_array.shape[0] * dcm_array.shape[1]  # Normalize sum of image\n",
    "    if train_or_val == \"train\":  # Only use train data to compute dataset statistics\n",
    "        sums += np.sum(dcm_array) / normalizer\n",
    "        sums_squared += (np.power(dcm_array, 2).sum()) / normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-income",
   "metadata": {},
   "source": [
    "Finally we store the train and val subject ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-attraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"Processed-Heart-Detection/train_subjects_det\", train_ids)\n",
    "np.save(\"Processed-Heart-Detection/val_subjects_det\", val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sums / len(train_ids)\n",
    "std = np.sqrt(sums_squared / len(train_ids) - (mean**2), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean of Dataset: {mean}, STD: {std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
